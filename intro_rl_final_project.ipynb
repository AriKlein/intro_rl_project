{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kGeLz_REsKzj"
      ],
      "authorship_tag": "ABX9TyPvqZ+jMhBYHt78MWQrBkGC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AriKlein/intro_rl_project/blob/main/intro_rl_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGeLz_REsKzj"
      },
      "source": [
        "# Introduction to Reinforcement Learning Final Project\n",
        "# Ari Klein (aeklein)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7iVz0l4GCCb"
      },
      "source": [
        "# 1)  Describe the methods and variables in the class DiscreteEnv which is the parent class of the Taxi V3 class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGNPxYW3GU9W"
      },
      "source": [
        "DiscreteEnv has member data:\n",
        "-\tnS and nA capturing the number of states and actions, respectively.  \n",
        "-\tisd, which is a list of length nS giving the probability, for each state, that the initial state gets set to that state for any given call to reset()\n",
        "-\tP, which is a dictionary storing, for each {state, action} pair, the corresponding next state, the corresponding reward, and whether, after taking that action from the initial state, the system is now in a terminal state.  If the transitions are probabilistic, it stores all possible sets of {next_state, reward, done} in a list along with the probability associated with each possible {next_state, reward, done}\n",
        "-\tlastaction, which stores the previous action taken\n",
        "-\ts, which stores the current state\n",
        "-\taction_space and observation_space, which are discrete spaces of size nA and nS, respectively, representing the space of possible actions and states, respectively. \n",
        "DiscreteEnv has member functions (e.g. methods):\n",
        "-\tinit for constructing a DiscreteEnv object with member data set to the values specified in its arguments\n",
        "-\tseed for setting the seed for the RNG to a specified value or to a random one if no value is specified\n",
        "-\treset, which resets the system by setting the state to an initial state sampled from the set of possible states according to distribution isd.  Also sets lastaction to None\n",
        "-\tstep, which takes an action, moves from the current state to the next state based on that action, and returns the next state, associated reward, and whether the system is now in a terminal state.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee0Tk3iaGnd2"
      },
      "source": [
        "DiscreteEnv has member functions (e.g. methods):\n",
        "-\tinit for constructing a DiscreteEnv object with member data set to the values specified in its arguments\n",
        "-\tseed for setting the seed for the RNG to a specified value or to a random one if no value is specified\n",
        "-\treset, which resets the system by setting the state to an initial state sampled from the set of possible states according to distribution isd.  Also sets lastaction to None\n",
        "-\tstep, which takes an action, moves from the current state to the next state based on that action, and returns the next state, associated reward, and whether the system is now in a terminal state.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94Rk2s8PriaG"
      },
      "source": [
        "# 2) Describe the methods and variables in the Taxi V3 class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2id-ybYTpya_"
      },
      "source": [
        "As TaxiEnv inherits DiscreteEnv, it has all the data and functions described above:\n",
        "-\tThe state captures the location on the map (out of 25 possible locations), the passenger location (5 total, including the possibility that the passenger is in the taxi), and the destination locations (out of 4 possible), for a total of nS = 25 X 5 X 4 = 500 possible states.\n",
        "-\tFrom each state, it is possible to move in any of 4 directions, or do a pickup, or do a dropoff, for a total of nA=6 possible actions.\n",
        "-\tThe initial state is a randomly chosen state from the 500 (= 5 X 5 X 5 X 4) possible states, subject to the constraint that the (initial) passenger location cannot be equal to the destination location (ruling out 5 X 5 X 4=100 possible states), nor can the passenger initially be inside the taxi, ruling out passenger location index 4, corresponding to 5 X 5 X 4 = 100 possible states.  This leaves 300 valid possible initial states, with the initial state being equally likely (e.g., with probability of 1/300) to be any one of these valid states.  isd is thus set to 1/300 for each of the 300 valid possible initial states, and 0 for the invalid initial states.\n",
        "-\tThe P dictionary captures the set of possible next states resulting from taking any action in any state, along with their associated probabilities, rewards, and whether they are terminal states.  Since the taxi environment is fully deterministic, there is only a single possible next state resulting from any action in any state, and it is assigned a probability of 1.  The only scenario resulting in a terminal state is performing a drop-off when the passenger is in the taxi and the taxi is at the correct destination.  Almost all actions in all states are assigned a reward of -1, with 3 exceptions:\n",
        " - Attempting a pickup when the passenger is either already in the taxi or not at the same location as the taxi is an illegal pickup and results in a reward of -10.\n",
        " - Attempting a drop-off when the passenger is either not in the taxi, or when the taxi is not in one of the four marked locations is an illegal drop-off and results in a reward of -10.\n",
        " - Performing a drop-off when the passenger is in the taxi and the taxi is at the correct destination, which results in a reward of 20.\n",
        "\n",
        "The new member data which are unique to the Taxi environment (e.g., not inherited from DiscreteEnv) are:\n",
        "-\tdesc, which captures the text-based map of the environment\n",
        "-\tlocs, which captures the possible pickup and dropoff locations as an array of ordered pairs on a 5X5 grid.\n",
        "\n",
        "The member functions are as follows:\n",
        "-\tinit sets the member data as described above.  \n",
        "-\treset resets the state of the environment to a random (uniformly distributed) state from the set of valid initial states, meaning that the initial taxi location, passenger location, and destination, are randomly chosen upon any call to reset.\n",
        "-\tstep takes an action, moves from the current state to the next state based on that action, and returns the next state, associated reward, and whether the system is now in a terminal state.  \n",
        "-\tencode converts a tuple containing the taxi row (0 to 4 going from north to south), taxi column (0 to 4 going from west to east), passenger location (index in locs, or 4 to indicate the passenger is in the taxi), and desired destination, and converts this tuple to a single number in [0,499].  In other words, it converts a logical representation of the state to a single integer representing the state.\n",
        "-\tdecode does the inverse of encode, namely converting a single integer representing the state into the corresponding taxi row, taxi column, passenger location and desired destination.\n",
        "-\trender depicts the environment and current state in an intuitive, human-readable, text-based format, where the taxi is shown on the map in its current location, with the taxi colored green if it has already successfully picked up the passenger, and yellow otherwise.  The location with the passenger (pre-pickup) is colored blue, while the desired destination is colored magenta.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhf7Lu7gryI2"
      },
      "source": [
        "# 3) Describe the Taxi V3 environment, its actions, states, reward structure and the rationale behind such a reward structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca-nWkQIsxPC"
      },
      "source": [
        "The Taxi V3 environment consists of a taxi agent which is initially at some random location on the below map:\n",
        "\n",
        "```\n",
        "MAP = [\n",
        "    \"+---------+\",\n",
        "    \"|R: | : :G|\",\n",
        "    \"| : | : : |\",\n",
        "    \"| : : : : |\",\n",
        "    \"| | : | : |\",\n",
        "    \"|Y| : |B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "```\n",
        "where the above map is illustrating a 5X5 grid.  The taxi can try to move north, east, west, or south, but will only be able to successfully move east or west through a `':'`.  There is a passenger randomly placed in the environment at one of the four location marked R, G, B, or Y, with a (different) desired destination that is randomly either R, G, B, or Y.  The goal of the taxi agent is to:\n",
        " - move to the passenger location\n",
        " - pick up the passenger at the passenger location, at which point the passenger is inside the taxi\n",
        " - move to the destination location\n",
        " - drop off the passenger at the destination location\n",
        "\n",
        "The taxi agent can always attempt to take one of 6 possible actions:\n",
        "- move south, north, east or west (actions 0-3, respectively)\n",
        "- pick up a passenger (action 4)\n",
        "- drop off a passenger (action 5)\n",
        "\n",
        "The state captures the location of the taxi agent on the map (out of 25 possible locations), the passenger location (5 total, including the possibility that the passenger is in the taxi), and the destination locations (out of 4 possible), for a total of 25 X 5 X 4 = 500 possible states.\n",
        "\n",
        "Almost all actions in all states are assigned a reward of -1, with 3 exceptions:\n",
        "- Attempting a pickup when the passenger is either already in the taxi or not at the same location as the taxi is an illegal pickup and results in a reward of -10.\n",
        "- Attempting a drop-off when the passenger is either not in the taxi, or when the taxi is not in one of the four marked locations is an illegal drop-off and results in a reward of -10.\n",
        "- Performing a drop-off when the passenger is in the taxi and the taxi is at the correct destination, which results in a reward of 20.\n",
        "\n",
        "The reward for successful completion of the final objective is to encourage the agent to successfully complete the objective; this reward needs to be effectively \"propagated\" backwards to encourage actions which lead the agent closer to acheiving the objective.  The rationale behind the default reward of -1 for almost all steps is to encourage the agent to acheive the goals described above in as few steps as possible (e.g., to encourage efficiency).  The relatively large penalties for illegal pickups and dropoffs are to strongly discourage the agent from taking such actions.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMk9aQY5Rb1t"
      },
      "source": [
        "# 4-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z37exgbBRtTn"
      },
      "source": [
        "Below are my implementations of SARSA and Q-learning, which both call my implementation of epsilon_greedy_action_from_Q.  The code for these three methods is largely based off the W3S2 and W4S1 solution branches (for example, https://github.com/KnowchowHQ/rl-in-action/blob/solution/C1-RL-Intro/W4S1/foolsball-v3.ipynb) which is in turn based off https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#sarsa-on-policy-td-control. \n",
        "My main changes to this code were:\n",
        " - The addition of hyperparameter `max_episode_length` for forcing episode end if taxi is looping\n",
        " - The addition of hyperparameter `num_exploration_episodes` to force exploration (epsilon=1) for several episodes before starting exploitation (epsilon decay)\n",
        " - The addition of hyperparameter `num_episodes_with_some_exploration` for forcing epsilon to 0 to force greedy policy for the purpose of calculating rubric metrics.\n",
        " - Calculating and returning the accumulated, non-discounted reward for each episode and checking whether each episode contains any illegal actions.  These are needed to evaluate algorithm performance against the rubric.  Since we should always be taking greedy actions fot the rubric, epsilon should be set to 0 for at least the last 1000 episodes (or 1100 to get 1000 sliding windows of length 100)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq3nP3w--Hbn"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gym\n",
        "\n",
        "# Given:\n",
        "# - a pandas table, Q, of discounted returns for for each action (columns) from each state (rows)\n",
        "# - a state\n",
        "# - an epsilon value\n",
        "# Returns an action sampled from a probability distribution wherein:\n",
        "# - The greedy action is taken with probability (1-epsilon) + epsilon/nA  (nA is number of possible actions)\n",
        "# - Any single non-greedy action can be taken with probability epsilon/nA  (nA is number of possible actions)\n",
        "# When epsilon=0, the below always gives the greedy action\n",
        "# When epsilon=1, the action is completely random (e.g., sampled from a uniform dist. across the possible actions)\n",
        "def epsilon_greedy_action_from_Q(Q, state, epsilon):\n",
        "    actions = Q.columns\n",
        "    action_probs = np.asarray([epsilon / len(actions)] * len(actions), dtype=np.float)\n",
        "\n",
        "    greedy_action_index = np.argmax(Q.loc[state].values)\n",
        "    action_probs[greedy_action_index] += 1 - epsilon\n",
        "\n",
        "    epsilon_greedy_action = np.random.choice(Q.columns, p=action_probs)\n",
        "\n",
        "    return epsilon_greedy_action\n",
        "\n",
        "# Implements sarsa, given a taxi agent and a dictionary of hyperparameters\n",
        "# Returns:\n",
        "#   - Pandas table of discounted returns for for each action (columns) from each state (rows)\n",
        "#   - numpy array of length n_episodes with reward for each epsiode.  Generally non-dsicounted but using epsilon-greedy policy.  By forcing epsilon to 0 for the last 1100 episodes, can get greedy rewards.\n",
        "#   - numpy array of length n_episodes with a '1' if the episode had at least 1 illegal action (action resulting in reward of -10), and a '0' if no illegal action was taken in the epsiode.\n",
        "def sarsa(taxi, HYPER_PARAMS):\n",
        "\n",
        "    actions_np = np.arange(taxi.nA)\n",
        "    states_np = np.arange(taxi.nS)\n",
        "\n",
        "    gamma = HYPER_PARAMS['gamma'] # parameter for discounted returns to encourage finding quickest route\n",
        "    n_episodes = HYPER_PARAMS['n_episodes']   # number of episodes to run\n",
        "    epsilon_decay = HYPER_PARAMS['epsilon_decay']   # controls exploration/exploitation tradeoff\n",
        "    min_epsilon = HYPER_PARAMS['min_epsilon']  # controls at what point to stop decaying epsilon (e.g., max exploitation we allow)\n",
        "    alpha = HYPER_PARAMS['alpha'] # effective learning rate telling us how to weight current return against value already in table\n",
        "\n",
        "    max_episode_length = HYPER_PARAMS['max_episode_length'] # max length of episode before forcing a reset in sarsa or q-learning\n",
        "    num_exploration_episodes = HYPER_PARAMS['num_exploration_episodes'] # num episodes to force explore before starting to decay epsilon\n",
        "    num_episodes_with_some_exploration = HYPER_PARAMS['num_episodes_with_some_exploration']\n",
        "\n",
        "    Q = pd.DataFrame.from_dict({s: {a: 0 for a in actions_np} for s in states_np}, orient='index')\n",
        "\n",
        "    epsilon = 1\n",
        "    rewards = np.zeros(n_episodes)\n",
        "    had_illegal_action = np.zeros(n_episodes) # 0 if no illegal action in the episode.  1 if had illegal action\n",
        "\n",
        "\n",
        "    for i in tqdm(range(n_episodes)):\n",
        "        s0 = taxi.reset()\n",
        "        a0 = epsilon_greedy_action_from_Q(Q, s0, epsilon)\n",
        "        #s0 = foolsball.init_state\n",
        "        done = False\n",
        "        current_episode_length=0\n",
        "        episode_reward = 0\n",
        "        episode_had_illegal_action = 0\n",
        "\n",
        "        while (not done) and (current_episode_length < max_episode_length):\n",
        "\n",
        "            s1, reward, done, dummy_prob = taxi.step(a0)\n",
        "            a1 = epsilon_greedy_action_from_Q(Q, s1, epsilon)\n",
        "            Q.loc[s0, a0] += alpha * (reward + gamma * Q.loc[s1, a1] - Q.loc[s0, a0])\n",
        "            s0 = s1\n",
        "            a0 = a1\n",
        "            current_episode_length = current_episode_length+1\n",
        "\n",
        "            # For the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "            # I will therefore force my epsilon to zero after a certain number of episodes, after which the below will be based on greedy\n",
        "            # actions instead of epsilon-greedy\n",
        "            episode_reward += reward\n",
        "            if reward == -10:\n",
        "                episode_had_illegal_action = 1\n",
        "\n",
        "        if i > num_exploration_episodes:\n",
        "            epsilon *= epsilon_decay\n",
        "            epsilon = max(epsilon, min_epsilon)\n",
        "\n",
        "        # for the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "        # I will therefore force my epsilon to zero after a certain number (num_episodes_with_some_exploration) of episodes.\n",
        "        # Will generally want this to be ~ 1100 less than total num episodes so that get at least 1100 episodes for rubric\n",
        "        if i > num_episodes_with_some_exploration:\n",
        "            epsilon = 0\n",
        "\n",
        "        rewards[i] = episode_reward\n",
        "        had_illegal_action[i] = episode_had_illegal_action\n",
        "\n",
        "    return Q, rewards, had_illegal_action\n",
        "\n",
        "# Implements Q-learning, given a taxi agent and a dictionary of hyperparameters\n",
        "# Returns:\n",
        "#   - Pandas table of discounted returns for for each action (columns) from each state (rows)\n",
        "#   - numpy array of length n_episodes with reward for each epsiode.  Generally non-dsicounted but using epsilon-greedy policy.  By forcing epsilon to 0 for the last 1100 episodes, can get greedy rewards.\n",
        "#   - numpy array of length n_episodes with a '1' if the episode had at least 1 illegal action (action resulting in reward of -10), and a '0' if no illegal action was taken in the epsiode.\n",
        "def q_learning(taxi, HYPER_PARAMS):\n",
        "\n",
        "    actions_np = np.arange(taxi.nA)\n",
        "    states_np = np.arange(taxi.nS)\n",
        "\n",
        "    gamma = HYPER_PARAMS['gamma'] # parameter for discounted returns to encourage finding quickest route\n",
        "    n_episodes = HYPER_PARAMS['n_episodes']   # number of episodes to run\n",
        "    epsilon_decay = HYPER_PARAMS['epsilon_decay']   # controls exploration/exploitation tradeoff\n",
        "    min_epsilon = HYPER_PARAMS['min_epsilon']  # controls at what point to stop decaying epsilon (e.g., max exploitation we allow)\n",
        "    alpha = HYPER_PARAMS['alpha'] # effective learning rate telling us how to weight current return against value already in table\n",
        "\n",
        "    max_episode_length = HYPER_PARAMS['max_episode_length'] # max length of episode before forcing a reset in sarsa or q-learning\n",
        "    num_exploration_episodes = HYPER_PARAMS['num_exploration_episodes'] # num episodes to force explore before starting to decay epsilon\n",
        "    num_episodes_with_some_exploration = HYPER_PARAMS['num_episodes_with_some_exploration']\n",
        "\n",
        "    Q = pd.DataFrame.from_dict({s: {a: 0 for a in actions_np} for s in states_np}, orient='index')\n",
        "\n",
        "    epsilon = 1\n",
        "    rewards = np.zeros(n_episodes)\n",
        "    had_illegal_action = np.zeros(n_episodes) # 0 if no illegal action in the episode.  1 if had illegal action\n",
        "\n",
        "\n",
        "    for i in tqdm(range(n_episodes)):\n",
        "        s0 = taxi.reset()\n",
        "        #s0 = foolsball.init_state\n",
        "        done = False\n",
        "        current_episode_length=0\n",
        "        episode_reward = 0\n",
        "        episode_had_illegal_action = 0\n",
        "\n",
        "        while (not done) and (current_episode_length < max_episode_length):\n",
        "            a0 = epsilon_greedy_action_from_Q(Q, s0, epsilon)\n",
        "            s1, reward, done, dummy_prob = taxi.step(a0)\n",
        "            Q.loc[s0, a0] += alpha * (reward + gamma * Q.loc[s1].max() - Q.loc[s0, a0])\n",
        "            s0 = s1\n",
        "            current_episode_length = current_episode_length+1\n",
        "\n",
        "            # For the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "            # I will therefore force my epsilon to zero after a certain number of episodes, after which the below will be based on greedy\n",
        "            # actions instead of epsilon-greedy\n",
        "            episode_reward += reward\n",
        "            if reward == -10:\n",
        "                episode_had_illegal_action = 1\n",
        "\n",
        "        if i > num_exploration_episodes:\n",
        "            epsilon *= epsilon_decay\n",
        "            epsilon = max(epsilon, min_epsilon)\n",
        "\n",
        "        # for the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "        # I will therefore force my epsilon to zero after a certain number (num_episodes_with_some_exploration) of episodes.\n",
        "        # Will generally want this to be ~ 1100 less than total num episodes so that get at least 1100 episodes for rubric\n",
        "        if i > num_episodes_with_some_exploration:\n",
        "            epsilon = 0\n",
        "\n",
        "        rewards[i] = episode_reward\n",
        "        had_illegal_action[i] = episode_had_illegal_action\n",
        "\n",
        "    return Q, rewards, had_illegal_action"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvYp9pqBX0gm"
      },
      "source": [
        "I defined the following 8 hyperparameters (description of each given in code comments):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21UnOhyKYLsG"
      },
      "source": [
        "MY_HYPER_PARAMS = {}\n",
        "MY_HYPER_PARAMS['gamma'] = 0.9 # parameter for discounted returns to encourage finding quickest route.\n",
        "MY_HYPER_PARAMS['n_episodes'] = 10000  # number of episodes to run\n",
        "MY_HYPER_PARAMS['epsilon_decay'] = 0.99  # controls exploration/exploitation tradeoff\n",
        "MY_HYPER_PARAMS['min_epsilon'] = 0.05  # controls at what point to stop decaying epsilon (e.g., max exploitation we allow)\n",
        "MY_HYPER_PARAMS['alpha'] = 0.1  # effective learning rate telling us how to weight current return against value already in table\n",
        "MY_HYPER_PARAMS['max_episode_length'] = 100 # max length of episode before forcing a reset in sarsa or q-learning\n",
        "MY_HYPER_PARAMS['num_exploration_episodes'] = 500  # num episodes to force explore before starting to decay epsilon\n",
        "\n",
        "# for the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "# I will therefore force my epsilon to zero after a certain number (num_episodes_with_some_exploration) of episodes.\n",
        "# Will generally want this to be ~ 1100 less than total num episodes so that get at least 1100 episodes for rubric\n",
        "MY_HYPER_PARAMS['num_episodes_with_some_exploration'] = MY_HYPER_PARAMS['n_episodes'] - 1102"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH-ljA46XZ2J"
      },
      "source": [
        "I preferred to work on my local machine in PyCharm for the enhanced debugging capabilities offered by that IDE.  My hyperparameter values resulted largely from trial-and-error in PyCharm, largely with the Q-learning algorithm, by inspection of the plot of windowed_rewards (see below) and the rubric values (e.g., 5th and 95th percentile rewards) for different hyperparameter settings.  For example:\n",
        "- I found that reducing `alpha` below 0.1 reduced convergence speed (e.g., I had to run for more episodes to get same result) without improving performance, so I am setting `alpha` to 0.1.\n",
        "- The `epsilon_decay value` of 0.99 implies that once we start allowing decay (after an initial exploration period of `num_exploration_episodes=500`) , it takes log(`min_epsilon`)/log(0.99) ~ 300 episodes to transition from full exploration to almost full exploitation. \n",
        "- Since there are 25 locations on the MAP, and we need to both pick up and drop off the passenger, it seems reasonable to set the `max_episode_length` to at least 50.  Since we initially expect the taxi to do some looping, I set it to 100.\n",
        "- The `num_episodes_with_some_exploration` is not really a hyperparameter.  I added it to force epsilon to 0 for the last 1100 episodes so that the last 1100 values in the vectors used for the rubric (e.g., the rewards vector and vector verifying that no illegal actions are taken) reflect episodes based entirely on greedy actions.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRXtsYHFbSv-",
        "outputId": "d92c8274-504c-4960-a2c1-f4c3579c36dc"
      },
      "source": [
        "# Define the MAP, initialize the taxi environment, and check that we can reset and render\n",
        "\n",
        "MAP = [\n",
        "    \"+---------+\",\n",
        "    \"|R: | : :G|\",\n",
        "    \"| : | : : |\",\n",
        "    \"| : : : : |\",\n",
        "    \"| | : | : |\",\n",
        "    \"|Y| : |B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "global_taxi = gym.make('Taxi-v3')\n",
        "global_taxi.reset()\n",
        "global_taxi.render()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "9AtsPln5dlmY",
        "outputId": "be21b61c-c2e5-4a4c-bcae-c40831541132"
      },
      "source": [
        "# Run Q-learning and check rubrics:\n",
        "estimated_returns_tbl, rewards, had_illegal_action = q_learning(global_taxi, MY_HYPER_PARAMS)\n",
        "\n",
        "# Calculate a moving average over a sliding window of 100 episodes \n",
        "windowed_rewards = np.convolve(rewards, np.ones(100)/100, 'valid')\n",
        "plt.plot(windowed_rewards[500:])\n",
        "plt.show()\n",
        "\n",
        "last_1000_start_idx = MY_HYPER_PARAMS['n_episodes'] - 100 - 1000\n",
        "avg_reward_last_1000 = windowed_rewards[last_1000_start_idx:(last_1000_start_idx+999)]\n",
        "\n",
        "avg_reward_5p = np.quantile(avg_reward_last_1000,.05)\n",
        "avg_reward_95p = np.quantile(avg_reward_last_1000,.95)\n",
        "\n",
        "print('5th percentile = '+str(avg_reward_5p))\n",
        "print('95th percentile = '+str(avg_reward_95p))\n",
        "print('Avg reward = '+str(np.mean(avg_reward_last_1000)))\n",
        "\n",
        "illegal_actions = np.count_nonzero(had_illegal_action[last_1000_start_idx:])\n",
        "print('Illegal actions in last 1000 = '+str(illegal_actions))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [03:09<00:00, 52.78it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "5th percentile = 7.540000000000001\n",
            "95th percentile = 8.560000000000002\n",
            "Avg reward = 8.00082082082082\n",
            "Illegal actions in last 1000 = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4hZq0F6fRSH"
      },
      "source": [
        "As seen above:\n",
        "- the 5th percentile is 7.54>7.2\n",
        "- the 95th percentile is 8.56>8.2\n",
        "- no illegal actions are taken in the last 1000 episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yjwsxJczX3b"
      },
      "source": [
        "# Appendix A: SARSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zYQnpzjzdFE"
      },
      "source": [
        "I largely optimized the hyperparameters for Q-learning, but found that running SARSA with the same hyperparameters also barely satisifies the rubric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "ZOGpVj_VzW4M",
        "outputId": "57caaee1-0faf-4d13-d05f-14886ae73e82"
      },
      "source": [
        "MY_HYPER_PARAMS = {}\n",
        "MY_HYPER_PARAMS['gamma'] = 0.9 # parameter for discounted returns to encourage finding quickest route.\n",
        "MY_HYPER_PARAMS['n_episodes'] = 10000  # number of episodes to run\n",
        "MY_HYPER_PARAMS['epsilon_decay'] = 0.99  # controls exploration/exploitation tradeoff\n",
        "MY_HYPER_PARAMS['min_epsilon'] = 0.05  # controls at what point to stop decaying epsilon (e.g., max exploitation we allow)\n",
        "MY_HYPER_PARAMS['alpha'] = 0.1  # effective learning rate telling us how to weight current return against value already in table\n",
        "MY_HYPER_PARAMS['max_episode_length'] = 100 # max length of episode before forcing a reset in sarsa or q-learning\n",
        "MY_HYPER_PARAMS['num_exploration_episodes'] = 500  # num episodes to force explore before starting to decay epsilon\n",
        "\n",
        "# for the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "# I will therefore force my epsilon to zero after a certain number (num_episodes_with_some_exploration) of episodes.\n",
        "# Will generally want this to be ~ 1100 less than total num episodes so that get at least 1100 episodes for rubric\n",
        "MY_HYPER_PARAMS['num_episodes_with_some_exploration'] = MY_HYPER_PARAMS['n_episodes'] - 1102\n",
        "\n",
        "# Run SARSA and check rubrics:\n",
        "estimated_returns_tbl, rewards, had_illegal_action = sarsa(global_taxi, MY_HYPER_PARAMS)\n",
        "\n",
        "# Calculate a moving average over a sliding window of 100 episodes \n",
        "windowed_rewards = np.convolve(rewards, np.ones(100)/100, 'valid')\n",
        "plt.plot(windowed_rewards[500:])\n",
        "plt.show()\n",
        "\n",
        "last_1000_start_idx = MY_HYPER_PARAMS['n_episodes'] - 100 - 1000\n",
        "avg_reward_last_1000 = windowed_rewards[last_1000_start_idx:(last_1000_start_idx+999)]\n",
        "\n",
        "avg_reward_5p = np.quantile(avg_reward_last_1000,.05)\n",
        "avg_reward_95p = np.quantile(avg_reward_last_1000,.95)\n",
        "\n",
        "print('5th percentile = '+str(avg_reward_5p))\n",
        "print('95th percentile = '+str(avg_reward_95p))\n",
        "print('Avg reward = '+str(np.mean(avg_reward_last_1000)))\n",
        "\n",
        "illegal_actions = np.count_nonzero(had_illegal_action[last_1000_start_idx:])\n",
        "print('Illegal actions in last 1000 = '+str(illegal_actions))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [02:21<00:00, 70.62it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "5th percentile = 7.24\n",
            "95th percentile = 8.23\n",
            "Avg reward = 7.705195195195195\n",
            "Illegal actions in last 1000 = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_kvsTdP22ZU"
      },
      "source": [
        "As seen above:\n",
        "- the 5th percentile is 7.24>7.2\n",
        "- the 95th percentile is 8.23>8.2\n",
        "- no illegal actions are taken in the last 1000 episodes\n",
        "- performance with SARSA is slightly worse than with Q-learning, but still barely satisfies the rubric.  Since it is by a razor-thin margin, it is likely that other runs would not necessarily satisfy the rubric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq8KpTtP7hgO"
      },
      "source": [
        "# Appendix B:  Visualization and Debugging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG-AA29O7qhq"
      },
      "source": [
        "I wrote several methods to aid in debugging and visualizing the tables and policies resulting from my implementations above.  I used these methods extensively in PyCharm, and have included them below for completeness.  For example, I wrote a method to pretty-print the returns table with more intuitive state and action labels, and to reorder the states in the table such that states with the same passenger and dest indices are grouped together.  I also wrote a method to pretty-print 8 policy maps: a policy map for each passenger location given that the passenger is not yet in the taxi, so that the taxi should be trying to get to and pick up the passenger, and a policy map for each destination location given that the passenger is already in the taxi, so the taxi should be trying to get to the destination and drop off the passenger.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWNv6KCJ7-Oq"
      },
      "source": [
        "# convert (row, col) to 1-base indices and convert pass and dest indices to intuitive strings\n",
        "def pretty_decode(local_taxi, s):\n",
        "    (row, col, pass_idx, dest_idx) = local_taxi.decode(s)\n",
        "    #pretty_row = row+1\n",
        "    #pretty_col = col+1\n",
        "    to_str ={0:'R', 1:'G', 2:'Y', 3:'B', 4:'T'}\n",
        "    return [row+1, col+1, to_str[pass_idx], to_str[dest_idx]]\n",
        "\n",
        "# Method to pretty-print the returns table:\n",
        "# - Using more intuitive state labels, column headings\n",
        "# - Reorder the states in the table such that states with the same passenger and dest indices are grouped together.\n",
        "def pretty_print_table(local_taxi, local_table):\n",
        "    new_table = local_table.copy()\n",
        "\n",
        "    # Reorder the states in the table such that states with the same passenger and dest indices are grouped together.\n",
        "    n_rows = local_table.shape[0]\n",
        "    stride = np.int32(n_rows/25)\n",
        "\n",
        "    frames = []\n",
        "    for ii in range(stride):\n",
        "        frames.append(new_table[ii::stride]) #.loc(list(np.arange(n_rows)[ii::stride])))\n",
        "        None\n",
        "\n",
        "    reorder_table = pd.concat(frames) #new_table[::stride] #.loc(list(np.arange(n_rows)[::stride]))\n",
        "\n",
        "\n",
        "    # Replace numbered rows with [row, col, pass loc, dest] where row and col are 1-based and pass loc and dest idx are in [RGBY]\n",
        "    pandas_state_pretty_replacer = {}\n",
        "    for s in range(local_taxi.nS):\n",
        "        pandas_state_pretty_replacer[s] = '['+ ','.join(\n",
        "            [str(x) for x in pretty_decode(local_taxi, s)])+']'  # tuple(global_taxi.decode(s))\n",
        "\n",
        "    reorder_table.rename(index=pandas_state_pretty_replacer, inplace=True)\n",
        "\n",
        "    # Rename column actions.   Use '+' for pickup and '-' for drop-off\n",
        "    reorder_table.columns = ['S', 'N', 'E', 'W', '+', '-']\n",
        "\n",
        "    print(reorder_table)\n",
        "    return reorder_table\n",
        "\n",
        "# Method to pretty-print the policies:\n",
        "# - Prints a policy map for each passenger location given that the passenger is not yet in the taxi, so the taxi should be trying to get to and pick up the passenger\n",
        "# - Prints a policy map for each destination location given that the passenger is already in the taxi, so the taxi should be trying to get to the destination and drop off the passenger\n",
        "def pretty_print_policy(taxi, local_policy):\n",
        "    direction_repr = {1:' 🡑 ', 2:' 🡒 ', 3:' 🡐 ', 0:' 🡓 ', 4:' + ', 5:' - ', None:' ⬤ '}\n",
        "\n",
        "    # Print policies for states where we are trying to get to passenger, so dest_idx is irrelevant, as long as not = pass_idx\n",
        "\n",
        "    print('Passenger not in taxi, pass at Red (top left):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 0, 1)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "    print('Passenger not in taxi, pass at Green (Top Right):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 1, 0)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "    print('Passenger not in taxi, pass at yellow (Bottom Left):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 2, 0)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "    print('Passenger not in taxi, pass at Blue (Bottom Right):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 3, 0)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "\n",
        "\n",
        "    # Print policies for states where we already have passenger and are trying to get to destination, so pass_idx is always 4\n",
        "\n",
        "    print('Passenger in taxi, Dest = Red (Top Left):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 4, 0)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "    print('Passenger in taxi, Dest = Green (Top Right):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 4, 1)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "    print('Passenger in taxi, Dest = Yellow (Bottom Left):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 4, 2)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "    print('Passenger in taxi, Dest = Blue (Bottom Right):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 4, 3)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ISd2W6Y-Fq2"
      },
      "source": [
        "Testing the above methods for the SARSA table and policy derived above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "q6ajMDTd-KCw",
        "outputId": "e56371c8-decf-4a4a-bf1a-d35f2405fea9"
      },
      "source": [
        "pretty_estimated_returns = pretty_print_table(global_taxi, estimated_returns_tbl)\n",
        "pretty_estimated_returns"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   S         N          E          W         +          -\n",
            "[1,1,R,R]   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000\n",
            "[1,2,R,R]   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000\n",
            "[1,3,R,R]   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000\n",
            "[1,4,R,R]   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000\n",
            "[1,5,R,R]   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000\n",
            "...              ...       ...        ...        ...       ...        ...\n",
            "[5,1,T,B]  -2.332281  4.344930  -1.086661  -2.871355 -9.959252  -4.549767\n",
            "[5,2,T,B]  -1.972276 -2.067928  -2.046950  -2.155782 -6.369106  -6.240870\n",
            "[5,3,T,B]  -2.169652 -2.062104  -2.165229  -2.101093 -6.469466  -5.102150\n",
            "[5,4,T,B]  15.893267  7.425521  10.654095  14.975301  6.997582  20.000000\n",
            "[5,5,T,B]   2.871712 -0.039446  -0.685568  16.090204 -5.665596  -1.229144\n",
            "\n",
            "[500 rows x 6 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>S</th>\n",
              "      <th>N</th>\n",
              "      <th>E</th>\n",
              "      <th>W</th>\n",
              "      <th>+</th>\n",
              "      <th>-</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>[1,1,R,R]</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>[1,2,R,R]</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>[1,3,R,R]</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>[1,4,R,R]</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>[1,5,R,R]</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>[5,1,T,B]</th>\n",
              "      <td>-2.332281</td>\n",
              "      <td>4.344930</td>\n",
              "      <td>-1.086661</td>\n",
              "      <td>-2.871355</td>\n",
              "      <td>-9.959252</td>\n",
              "      <td>-4.549767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>[5,2,T,B]</th>\n",
              "      <td>-1.972276</td>\n",
              "      <td>-2.067928</td>\n",
              "      <td>-2.046950</td>\n",
              "      <td>-2.155782</td>\n",
              "      <td>-6.369106</td>\n",
              "      <td>-6.240870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>[5,3,T,B]</th>\n",
              "      <td>-2.169652</td>\n",
              "      <td>-2.062104</td>\n",
              "      <td>-2.165229</td>\n",
              "      <td>-2.101093</td>\n",
              "      <td>-6.469466</td>\n",
              "      <td>-5.102150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>[5,4,T,B]</th>\n",
              "      <td>15.893267</td>\n",
              "      <td>7.425521</td>\n",
              "      <td>10.654095</td>\n",
              "      <td>14.975301</td>\n",
              "      <td>6.997582</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>[5,5,T,B]</th>\n",
              "      <td>2.871712</td>\n",
              "      <td>-0.039446</td>\n",
              "      <td>-0.685568</td>\n",
              "      <td>16.090204</td>\n",
              "      <td>-5.665596</td>\n",
              "      <td>-1.229144</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   S         N          E          W         +          -\n",
              "[1,1,R,R]   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000\n",
              "[1,2,R,R]   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000\n",
              "[1,3,R,R]   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000\n",
              "[1,4,R,R]   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000\n",
              "[1,5,R,R]   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000\n",
              "...              ...       ...        ...        ...       ...        ...\n",
              "[5,1,T,B]  -2.332281  4.344930  -1.086661  -2.871355 -9.959252  -4.549767\n",
              "[5,2,T,B]  -1.972276 -2.067928  -2.046950  -2.155782 -6.369106  -6.240870\n",
              "[5,3,T,B]  -2.169652 -2.062104  -2.165229  -2.101093 -6.469466  -5.102150\n",
              "[5,4,T,B]  15.893267  7.425521  10.654095  14.975301  6.997582  20.000000\n",
              "[5,5,T,B]   2.871712 -0.039446  -0.685568  16.090204 -5.665596  -1.229144\n",
              "\n",
              "[500 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mihS9jdQ-iEu",
        "outputId": "c230d92e-7d6b-47d6-85e6-669619e062b9"
      },
      "source": [
        "# get the greedy policy for each state by looking in the returns table for the action with max return for that state\n",
        "def greedy_policy_from_returns_tbl(local_table):\n",
        "    policy = {s: None for s in local_table.index}\n",
        "    for local_state in local_table.index:\n",
        "            greedy_action = local_table.loc[local_state].idxmax()\n",
        "            policy[local_state] = greedy_action\n",
        "    return policy\n",
        "\n",
        "greedy_policy = greedy_policy_from_returns_tbl(estimated_returns_tbl)\n",
        "print(greedy_policy)\n",
        "pretty_print_policy(global_taxi,greedy_policy)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 0, 1: 4, 2: 4, 3: 4, 4: 2, 5: 0, 6: 0, 7: 2, 8: 0, 9: 0, 10: 0, 11: 2, 12: 0, 13: 0, 14: 0, 15: 0, 16: 5, 17: 2, 18: 0, 19: 0, 20: 0, 21: 3, 22: 3, 23: 0, 24: 0, 25: 0, 26: 3, 27: 0, 28: 3, 29: 0, 30: 0, 31: 0, 32: 0, 33: 3, 34: 3, 35: 0, 36: 3, 37: 0, 38: 3, 39: 0, 40: 0, 41: 2, 42: 0, 43: 0, 44: 2, 45: 0, 46: 2, 47: 2, 48: 2, 49: 0, 50: 0, 51: 0, 52: 2, 53: 2, 54: 0, 55: 0, 56: 0, 57: 2, 58: 2, 59: 2, 60: 0, 61: 0, 62: 3, 63: 3, 64: 2, 65: 0, 66: 2, 67: 2, 68: 2, 69: 3, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 3, 77: 2, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 4, 85: 0, 86: 4, 87: 4, 88: 0, 89: 0, 90: 0, 91: 3, 92: 3, 93: 3, 94: 0, 95: 0, 96: 3, 97: 5, 98: 3, 99: 0, 100: 0, 101: 1, 102: 1, 103: 1, 104: 1, 105: 0, 106: 0, 107: 0, 108: 0, 109: 0, 110: 0, 111: 0, 112: 0, 113: 0, 114: 0, 115: 0, 116: 1, 117: 2, 118: 0, 119: 0, 120: 0, 121: 3, 122: 3, 123: 3, 124: 0, 125: 0, 126: 0, 127: 0, 128: 3, 129: 0, 130: 0, 131: 0, 132: 0, 133: 0, 134: 3, 135: 0, 136: 3, 137: 0, 138: 0, 139: 0, 140: 0, 141: 2, 142: 0, 143: 0, 144: 2, 145: 0, 146: 1, 147: 2, 148: 0, 149: 0, 150: 0, 151: 0, 152: 2, 153: 0, 154: 2, 155: 0, 156: 0, 157: 1, 158: 0, 159: 2, 160: 0, 161: 0, 162: 2, 163: 3, 164: 1, 165: 0, 166: 1, 167: 1, 168: 0, 169: 2, 170: 0, 171: 3, 172: 0, 173: 0, 174: 0, 175: 0, 176: 3, 177: 2, 178: 0, 179: 0, 180: 0, 181: 3, 182: 0, 183: 0, 184: 1, 185: 0, 186: 1, 187: 1, 188: 0, 189: 0, 190: 0, 191: 3, 192: 3, 193: 0, 194: 0, 195: 0, 196: 3, 197: 1, 198: 3, 199: 3, 200: 0, 201: 1, 202: 1, 203: 1, 204: 1, 205: 0, 206: 2, 207: 2, 208: 0, 209: 0, 210: 0, 211: 0, 212: 2, 213: 2, 214: 2, 215: 0, 216: 1, 217: 2, 218: 0, 219: 2, 220: 0, 221: 1, 222: 3, 223: 1, 224: 2, 225: 0, 226: 2, 227: 2, 228: 3, 229: 3, 230: 0, 231: 3, 232: 2, 233: 2, 234: 2, 235: 0, 236: 1, 237: 2, 238: 3, 239: 2, 240: 0, 241: 3, 242: 3, 243: 3, 244: 1, 245: 0, 246: 1, 247: 2, 248: 3, 249: 3, 250: 0, 251: 3, 252: 1, 253: 2, 254: 2, 255: 0, 256: 3, 257: 1, 258: 3, 259: 2, 260: 0, 261: 3, 262: 3, 263: 3, 264: 1, 265: 0, 266: 1, 267: 1, 268: 3, 269: 3, 270: 0, 271: 3, 272: 0, 273: 0, 274: 0, 275: 0, 276: 3, 277: 2, 278: 3, 279: 0, 280: 0, 281: 3, 282: 3, 283: 3, 284: 1, 285: 0, 286: 1, 287: 1, 288: 0, 289: 3, 290: 0, 291: 3, 292: 0, 293: 3, 294: 0, 295: 0, 296: 3, 297: 1, 298: 3, 299: 0, 300: 0, 301: 1, 302: 1, 303: 1, 304: 1, 305: 0, 306: 1, 307: 1, 308: 0, 309: 0, 310: 0, 311: 0, 312: 1, 313: 1, 314: 1, 315: 0, 316: 1, 317: 1, 318: 0, 319: 1, 320: 0, 321: 1, 322: 1, 323: 1, 324: 1, 325: 0, 326: 1, 327: 1, 328: 1, 329: 2, 330: 0, 331: 1, 332: 2, 333: 1, 334: 0, 335: 0, 336: 1, 337: 1, 338: 1, 339: 1, 340: 0, 341: 1, 342: 1, 343: 1, 344: 1, 345: 0, 346: 1, 347: 3, 348: 3, 349: 1, 350: 0, 351: 3, 352: 1, 353: 1, 354: 1, 355: 0, 356: 3, 357: 1, 358: 1, 359: 1, 360: 0, 361: 1, 362: 2, 363: 2, 364: 1, 365: 0, 366: 1, 367: 1, 368: 1, 369: 1, 370: 0, 371: 1, 372: 0, 373: 0, 374: 0, 375: 0, 376: 1, 377: 2, 378: 1, 379: 0, 380: 0, 381: 1, 382: 1, 383: 1, 384: 3, 385: 0, 386: 3, 387: 3, 388: 3, 389: 1, 390: 0, 391: 3, 392: 0, 393: 0, 394: 0, 395: 0, 396: 1, 397: 1, 398: 1, 399: 0, 400: 0, 401: 1, 402: 1, 403: 1, 404: 1, 405: 0, 406: 1, 407: 1, 408: 4, 409: 4, 410: 0, 411: 4, 412: 1, 413: 1, 414: 1, 415: 0, 416: 1, 417: 1, 418: 5, 419: 1, 420: 0, 421: 1, 422: 2, 423: 2, 424: 2, 425: 0, 426: 1, 427: 1, 428: 1, 429: 1, 430: 0, 431: 1, 432: 2, 433: 2, 434: 2, 435: 0, 436: 0, 437: 3, 438: 1, 439: 0, 440: 0, 441: 3, 442: 1, 443: 1, 444: 1, 445: 0, 446: 3, 447: 3, 448: 3, 449: 1, 450: 0, 451: 3, 452: 1, 453: 1, 454: 1, 455: 0, 456: 2, 457: 3, 458: 3, 459: 1, 460: 0, 461: 1, 462: 2, 463: 2, 464: 1, 465: 0, 466: 1, 467: 1, 468: 1, 469: 1, 470: 0, 471: 1, 472: 4, 473: 4, 474: 4, 475: 0, 476: 1, 477: 1, 478: 1, 479: 5, 480: 0, 481: 1, 482: 1, 483: 1, 484: 1, 485: 0, 486: 1, 487: 3, 488: 1, 489: 1, 490: 0, 491: 1, 492: 3, 493: 3, 494: 3, 495: 0, 496: 1, 497: 1, 498: 3, 499: 3}\n",
            "Passenger not in taxi, pass at Red (top left):\n",
            "| + : 🡐 | 🡒 : 🡓 : 🡓 \n",
            "| 🡑 : 🡐 | 🡒 : 🡓 : 🡐 \n",
            "| 🡑 : 🡑 : 🡐 : 🡐 : 🡐 \n",
            "| 🡑 | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "| 🡑 | 🡑 : 🡐 | 🡑 : 🡑 \n",
            "Passenger not in taxi, pass at Green (Top Right):\n",
            "| 🡒 : 🡓 | 🡒 : 🡒 : + \n",
            "| 🡑 : 🡓 | 🡒 : 🡑 : 🡑 \n",
            "| 🡑 : 🡒 : 🡑 : 🡑 : 🡑 \n",
            "| 🡑 | 🡑 : 🡑 | 🡑 : 🡐 \n",
            "| 🡑 | 🡒 : 🡑 | 🡑 : 🡑 \n",
            "Passenger not in taxi, pass at yellow (Bottom Left):\n",
            "| 🡓 : 🡐 | 🡒 : 🡒 : 🡓 \n",
            "| 🡓 : 🡐 | 🡓 : 🡓 : 🡓 \n",
            "| 🡓 : 🡐 : 🡐 : 🡐 : 🡓 \n",
            "| 🡓 | 🡑 : 🡐 | 🡑 : 🡐 \n",
            "| + | 🡑 : 🡐 | 🡑 : 🡑 \n",
            "Passenger not in taxi, pass at Blue (Bottom Right):\n",
            "| 🡓 : 🡓 | 🡒 : 🡓 : 🡐 \n",
            "| 🡓 : 🡓 | 🡒 : 🡓 : 🡐 \n",
            "| 🡒 : 🡒 : 🡑 : 🡓 : 🡓 \n",
            "| 🡑 | 🡒 : 🡑 | 🡓 : 🡓 \n",
            "| 🡑 | 🡒 : 🡑 | + : 🡐 \n",
            "Passenger in taxi, Dest = Red (Top Left):\n",
            "| - : 🡐 | 🡓 : 🡐 : 🡐 \n",
            "| 🡑 : 🡐 | 🡓 : 🡐 : 🡐 \n",
            "| 🡑 : 🡑 : 🡐 : 🡐 : 🡐 \n",
            "| 🡑 | 🡑 : 🡐 | 🡑 : 🡑 \n",
            "| 🡑 | 🡓 : 🡒 | 🡑 : 🡑 \n",
            "Passenger in taxi, Dest = Green (Top Right):\n",
            "| 🡒 : 🡓 | 🡒 : 🡒 : - \n",
            "| 🡒 : 🡓 | 🡑 : 🡒 : 🡑 \n",
            "| 🡒 : 🡒 : 🡑 : 🡒 : 🡑 \n",
            "| 🡑 | 🡑 : 🡑 | 🡒 : 🡑 \n",
            "| 🡑 | 🡐 : 🡐 | 🡑 : 🡑 \n",
            "Passenger in taxi, Dest = Yellow (Bottom Left):\n",
            "| 🡓 : 🡐 | 🡒 : 🡓 : 🡐 \n",
            "| 🡓 : 🡓 | 🡓 : 🡓 : 🡐 \n",
            "| 🡓 : 🡐 : 🡐 : 🡐 : 🡐 \n",
            "| 🡓 | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "| - | 🡑 : 🡐 | 🡑 : 🡐 \n",
            "Passenger in taxi, Dest = Blue (Bottom Right):\n",
            "| 🡓 : 🡓 | 🡒 : 🡓 : 🡓 \n",
            "| 🡓 : 🡓 | 🡒 : 🡓 : 🡐 \n",
            "| 🡒 : 🡒 : 🡒 : 🡓 : 🡓 \n",
            "| 🡑 | 🡑 : 🡑 | 🡓 : 🡓 \n",
            "| 🡑 | 🡓 : 🡑 | - : 🡐 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qozKVpZ_2eV"
      },
      "source": [
        "As seen, the agent has successfully learned not to take any illegal actions.  It is not always taking the shortest possible path (which is probably why the q-learning policy does a better job on the rubrics), but it is generally taking a decent path to complete the objective. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeslesM7Iqyh"
      },
      "source": [
        "# Appendix C: State Compression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHBZ22I-IzVm"
      },
      "source": [
        "Inspired by the discussions regarding Hierarchical RL in https://arxiv.org/abs/cs/9905014, I observed that when we consider the empty taxi attempting to find and pick up the passenger, the final destination is irrelevant.  This led me to consider compressing the state space from 5 X 5 X 5 X 4 = 500 states to only 5 X 5 X 2 X 4 = 200 states, where for each of the 25 locations on the map, I can either have an empty or a full taxi, and a single destination in {R, G, B, Y}, where that destination is given by the passenger location when the taxi is empty, and the final destination when the taxi is full.  To implement this, I wrote methods to convert the original state to a compressed state, and to encode and decode compressed states.  I still pass the original state to the taxi step() API, but I use compressed states when populating the table, which is now compressed.  \n",
        "\n",
        "For the actions, I similarly noted that since the agent is aware of whether it is empty or full, it should never consider doing drop-offs when empty and should never consider doing pick-ups when full, so it is always choosing from 1 of 5 (as opposed to 6) actions:  4 directions and a single combined pickup/dropoff action.\n",
        "\n",
        "The resulting, modified Q-learning and helper functions are thus as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "K8RiFkbFK869",
        "outputId": "e36c6ad5-811f-41fa-ca09-d8a7bc0b92e6"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gym\n",
        "\n",
        "\n",
        "# convert (row, col) to 1-base indices and convert pass and dest indices to intuitive strings\n",
        "def pretty_decode(local_taxi, s):\n",
        "    (row, col, pass_idx, dest_idx) = local_taxi.decode(s)\n",
        "    #pretty_row = row+1\n",
        "    #pretty_col = col+1\n",
        "    to_str ={0:'R', 1:'G', 2:'Y', 3:'B', 4:'T'}\n",
        "    return [row+1, col+1, to_str[pass_idx], to_str[dest_idx]]\n",
        "\n",
        "def pretty_decode_compressed(local_taxi, compressed_state):\n",
        "    (row, col, pass_in_taxi, dest_idx) = decode_compressed_state(compressed_state)\n",
        "    #pretty_row = row+1\n",
        "    #pretty_col = col+1\n",
        "    to_str_pass = {0:'-', 1:'+'}\n",
        "    to_str_dest = {0:'R', 1:'G', 2:'Y', 3:'B', 4:'T'}\n",
        "    return [row+1, col+1, to_str_pass[pass_in_taxi], to_str_dest[dest_idx]]\n",
        "\n",
        "def epsilon_greedy_action_from_Q(Q, state, epsilon):\n",
        "    actions = Q.columns\n",
        "    action_probs = np.asarray([epsilon / len(actions)] * len(actions), dtype=np.float)\n",
        "\n",
        "    greedy_action_index = np.argmax(Q.loc[state].values)\n",
        "    action_probs[greedy_action_index] += 1 - epsilon\n",
        "\n",
        "    epsilon_greedy_action = np.random.choice(Q.columns, p=action_probs)\n",
        "\n",
        "    return epsilon_greedy_action\n",
        "\n",
        "'''\n",
        "def epsilon_greedy_action_compressed(Q, compressed_state, epsilon):\n",
        "    actions = Q.columns\n",
        "    action_probs = np.asarray([epsilon / len(actions)] * len(actions), dtype=np.float)\n",
        "\n",
        "    greedy_action_index = np.argmax(Q.loc[compressed_state].values)\n",
        "    action_probs[greedy_action_index] += 1 - epsilon\n",
        "\n",
        "    epsilon_greedy_action = np.random.choice(Q.columns, p=action_probs)\n",
        "\n",
        "    return epsilon_greedy_action\n",
        "'''\n",
        "\n",
        "# state compression\n",
        "\n",
        "def encode_compressed_state(taxi_row, taxi_col, pass_loc, dest_idx):\n",
        "    # (5) 5, 2, 4\n",
        "    i = taxi_row\n",
        "    i *= 5\n",
        "    i += taxi_col\n",
        "    i *= 2 # passenger is either in taxi or not in taxi\n",
        "    i += pass_loc\n",
        "    i *= 4\n",
        "    i += dest_idx  # passenger loc if passenger not in taxi, else dest loc\n",
        "    return i\n",
        "\n",
        "def decode_compressed_state(i):\n",
        "    dest_idx = i%4\n",
        "    i = i//4\n",
        "    pass_in_taxi = i%2\n",
        "    i = i//2\n",
        "    col_idx = i%5\n",
        "    i = i//5\n",
        "    row_idx = i%5\n",
        "    assert 0 <= i < 5\n",
        "    return row_idx, col_idx, pass_in_taxi, dest_idx\n",
        "\n",
        "def compress_state(local_taxi, original_state):\n",
        "    (row, col, pass_idx, dest_idx) = local_taxi.decode(original_state)\n",
        "    if(pass_idx==4):\n",
        "        pass_in_taxi = 1\n",
        "    else:  # not yet in taxi, so destination is irrelevant.  we should use the passenger index as the current destination\n",
        "        pass_in_taxi = 0\n",
        "        dest_idx = pass_idx\n",
        "    return encode_compressed_state(row, col, pass_in_taxi, dest_idx)\n",
        "\n",
        "def decompress_state(local_taxi, compressed_state):\n",
        "    (row, col, pass_in_taxi, current_dest_idx) = decode_compressed_state(compressed_state)\n",
        "    if (pass_in_taxi == 1):\n",
        "        return local_taxi.encode(row, col, 4, current_dest_idx)\n",
        "    else:  # not yet in taxi, so destination is irrelevant.  The current destination is the pass_idx.  dest_idx is irrelevant and just needs to not equal pass_idx\n",
        "        return local_taxi.encode(row, col, current_dest_idx, (current_dest_idx+1) %4)\n",
        "\n",
        "# Exploit fact that destination is irrelevant during pickup by combining knowledge for all dest_idx when pass_idx!=4\n",
        "# Never do dropoff when pass_idx!=4\n",
        "# Never do pickup when pass_idx=4\n",
        "def q_learning_combine_states(taxi, HYPER_PARAMS):\n",
        "\n",
        "    actions_np = np.arange(5) # only 5 actions since will force no pickup when pass in taxi and no dropoff otherwise\n",
        "    states_np = np.arange(5*5*2*4) # compress to 25*8=200 states by using a single destination which is set to passenger loc when passenger not in taxi and to dest when passenger in taxi\n",
        "\n",
        "    gamma = HYPER_PARAMS['gamma'] # parameter for discounted returns to encourage finding quickest route\n",
        "    n_episodes = HYPER_PARAMS['n_episodes']   # number of episodes to run\n",
        "    epsilon_decay = HYPER_PARAMS['epsilon_decay']   # controls exploration/exploitation tradeoff\n",
        "    min_epsilon = HYPER_PARAMS['min_epsilon']  # controls at what point to stop decaying epsilon (e.g., max exploitation we allow)\n",
        "    alpha = HYPER_PARAMS['alpha'] # effective learning rate telling us how to weight current return against value already in table\n",
        "\n",
        "    max_episode_length = HYPER_PARAMS['max_episode_length'] # max length of episode before forcing a reset in sarsa or q-learning\n",
        "    num_exploration_episodes = HYPER_PARAMS['num_exploration_episodes'] # num episodes to force explore before starting to decay epsilon\n",
        "    num_episodes_with_some_exploration = HYPER_PARAMS['num_episodes_with_some_exploration']\n",
        "\n",
        "    Q = pd.DataFrame.from_dict({s: {a: 0 for a in actions_np} for s in states_np}, orient='index')\n",
        "\n",
        "    epsilon = 1\n",
        "    rewards = np.zeros(n_episodes)\n",
        "    had_illegal_action = np.zeros(n_episodes) # 0 if no illegal action in the episode.  1 if had illegal action\n",
        "\n",
        "    for i in tqdm(range(n_episodes)):\n",
        "        s0 = taxi.reset()\n",
        "        compressed_s0 = compress_state(taxi, s0)\n",
        "        #s0 = foolsball.init_state\n",
        "        done = False\n",
        "        current_episode_length=0\n",
        "        episode_reward = 0\n",
        "        episode_had_illegal_action = 0\n",
        "\n",
        "        while (not done) and (current_episode_length < max_episode_length):\n",
        "\n",
        "            a0 = epsilon_greedy_action_from_Q(Q, compressed_s0, epsilon) # will be 0->4 since only have 5 actions\n",
        "            (row_idx, col_idx, pass_in_taxi, dest_idx) = decode_compressed_state(compressed_s0)\n",
        "            #(row, col, pass_pretty, dest_pretty) = pretty_decode(taxi, s0)\n",
        "\n",
        "            if a0 == 4:  # select pickup/dropoff based on state\n",
        "                if pass_in_taxi == 1:  # passenger in taxi, always do dropoff, never pickup\n",
        "                    s1, reward, done, dummy_prob = taxi.step(5)\n",
        "                else:  # passenger not in taxi, always do pickup, never dropoff\n",
        "                    s1, reward, done, dummy_prob = taxi.step(4)\n",
        "            else: # do standard directional action\n",
        "                s1, reward, done, dummy_prob = taxi.step(a0)\n",
        "\n",
        "            compressed_s1 = compress_state(taxi, s1)\n",
        "\n",
        "            Q.loc[compressed_s0, a0] += alpha * (reward + gamma * Q.loc[compressed_s1].max() - Q.loc[compressed_s0, a0])\n",
        "            compressed_s0 = compressed_s1\n",
        "            current_episode_length = current_episode_length+1\n",
        "\n",
        "            # For the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "            # I will therefore force my epsilon to zero after a certain number of episodes, after which the below will be based on greedy\n",
        "            # actions instead of epsilon-greedy\n",
        "            episode_reward += reward\n",
        "            if reward == -10:\n",
        "                episode_had_illegal_action = 1\n",
        "\n",
        "        if i > num_exploration_episodes:\n",
        "            epsilon *= epsilon_decay\n",
        "            epsilon = max(epsilon, min_epsilon)\n",
        "\n",
        "        # for the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "        # I will therefore force my epsilon to zero after a certain number (num_episodes_with_some_exploration) of episodes.\n",
        "        # Will generally want this to be ~ 1100 less than total num episodes so that get at least 1100 episodes for rubric\n",
        "        if i > num_episodes_with_some_exploration:\n",
        "            epsilon = 0\n",
        "\n",
        "        rewards[i] = episode_reward\n",
        "        had_illegal_action[i] = episode_had_illegal_action\n",
        "\n",
        "    return Q, rewards, had_illegal_action\n",
        "\n",
        "\n",
        "def pretty_print_table(local_taxi, local_table):\n",
        "    new_table = local_table.copy()\n",
        "    num_states = 5*5*2*4\n",
        "\n",
        "    # Reorder the states in the table such that states with the same passenger and dest indices are grouped together.\n",
        "    n_rows = local_table.shape[0]\n",
        "    stride = np.int32(n_rows/25)\n",
        "\n",
        "    frames = []\n",
        "    for ii in range(stride):\n",
        "        frames.append(new_table[ii::stride]) #.loc(list(np.arange(n_rows)[ii::stride])))\n",
        "        None\n",
        "\n",
        "    reorder_table = pd.concat(frames) #new_table[::stride] #.loc(list(np.arange(n_rows)[::stride]))\n",
        "\n",
        "\n",
        "    # Replace numbered rows with [row, col, pass loc, dest] where row and col are 1-based and pass loc and dest idx are in [RGBY]\n",
        "    pandas_state_pretty_replacer = {}\n",
        "    for s in range(num_states):\n",
        "        pandas_state_pretty_replacer[s] = '['+ ','.join(\n",
        "            [str(x) for x in pretty_decode_compressed(local_taxi, s)])+']'  # tuple(global_taxi.decode(s))\n",
        "\n",
        "    reorder_table.rename(index=pandas_state_pretty_replacer, inplace=True)\n",
        "\n",
        "    # Rename column actions.   Use '+' for pickup and '-' for drop-off\n",
        "    reorder_table.columns = ['S', 'N', 'E', 'W', '+/-']\n",
        "\n",
        "    print(reorder_table)\n",
        "    return reorder_table\n",
        "\n",
        "def greedy_policy_from_returns_tbl(local_table):\n",
        "    policy = {s: None for s in local_table.index}\n",
        "    for local_state in local_table.index:\n",
        "            greedy_action = local_table.loc[local_state].idxmax()\n",
        "            policy[local_state] = greedy_action\n",
        "    return policy\n",
        "\n",
        "# set up hyper-parameters\n",
        "MY_HYPER_PARAMS = {}\n",
        "MY_HYPER_PARAMS['gamma'] = 0.9 # parameter for discounted returns to encourage finding quickest route.\n",
        "MY_HYPER_PARAMS['n_episodes'] = 5000  # number of episodes to run\n",
        "MY_HYPER_PARAMS['epsilon_decay'] = 0.99  # controls exploration/exploitation tradeoff\n",
        "MY_HYPER_PARAMS['min_epsilon'] = 0.05  # controls at what point to stop decaying epsilon (e.g., max exploitation we allow)\n",
        "MY_HYPER_PARAMS['alpha'] = 0.1  # effective learning rate telling us how to weight current return against value already in table\n",
        "MY_HYPER_PARAMS['max_episode_length'] = 50 # max length of episode before forcing a reset in sarsa or q-learning\n",
        "MY_HYPER_PARAMS['num_exploration_episodes'] = 500  # num episodes to force explore before starting to decay epsilon\n",
        "\n",
        "# for the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "# I will therefore force my epsilon to zero after a certain number (num_episodes_with_some_exploration) of episodes.\n",
        "# Will generally want this to be ~ 1100 less than total num episodes so that get at least 1100 episodes for rubric\n",
        "MY_HYPER_PARAMS['num_episodes_with_some_exploration'] = MY_HYPER_PARAMS['n_episodes'] - 1102\n",
        "\n",
        "global_taxi = gym.make('Taxi-v3')\n",
        "\n",
        "estimated_returns_tbl, rewards, had_illegal_action = q_learning_combine_states(global_taxi, MY_HYPER_PARAMS)\n",
        "\n",
        "pretty_estimated_returns = pretty_print_table(global_taxi, estimated_returns_tbl)\n",
        "\n",
        "windowed_rewards = np.convolve(rewards, np.ones(100)/100, 'valid')\n",
        "plt.plot(windowed_rewards[500:])\n",
        "plt.show()\n",
        "\n",
        "last_1000_start_idx = MY_HYPER_PARAMS['n_episodes'] - 100 - 1000\n",
        "avg_reward_last_1000 = windowed_rewards[last_1000_start_idx:(last_1000_start_idx+999)]\n",
        "\n",
        "avg_reward_5p = np.quantile(avg_reward_last_1000,.05)\n",
        "avg_reward_95p = np.quantile(avg_reward_last_1000,.95)\n",
        "\n",
        "print('5th percentile = '+str(avg_reward_5p))\n",
        "print('95th percentile = '+str(avg_reward_95p))\n",
        "print('Avg reward = '+str(np.mean(avg_reward_last_1000)))\n",
        "\n",
        "illegal_actions = np.count_nonzero(had_illegal_action[last_1000_start_idx:])\n",
        "print('Illegal actions in last 1000 = '+str(illegal_actions))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:28<00:00, 56.37it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                   S          N         E          W        +/-\n",
            "[1,1,-,R]   0.503471   1.371662 -0.621951   2.048661   6.849698\n",
            "[1,2,-,R]  -0.329433   0.065785 -1.379739   4.906872 -10.279443\n",
            "[1,3,-,R]  -1.636542  -5.709761 -5.741375  -5.705112 -12.618674\n",
            "[1,4,-,R]  -2.059947  -5.850446 -5.845570  -5.508752 -12.524174\n",
            "[1,5,-,R]  -5.983507  -5.972340 -5.992501  -3.276214 -13.217079\n",
            "...              ...        ...       ...        ...        ...\n",
            "[5,1,+,B]  -1.011996   7.168689 -1.177035  -2.275347  -1.558016\n",
            "[5,2,+,B]  -1.224790  -1.178931 -1.232774  -1.224790  -1.990000\n",
            "[5,3,+,B]  -1.216732  -0.796313 -1.216732  -1.205525  -1.900000\n",
            "[5,4,+,B]  12.187942  10.110043  7.196194  13.412220  25.858181\n",
            "[5,5,+,B]   0.000000   0.698188  0.000000  20.828247   0.000000\n",
            "\n",
            "[200 rows x 5 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "5th percentile = 7.348999999999999\n",
            "95th percentile = 8.34\n",
            "Avg reward = 7.916956956956957\n",
            "Illegal actions in last 1000 = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC8b3sx0L11k"
      },
      "source": [
        "As seen, the state and action spaces have been successfully compressed to 200 and 5, respectively, and we are still getting a good policy which passes the rubric.  Convergence also appears to be faster, which is as expected due to the reduced space of possible states and actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7JMdwUtMEOJ"
      },
      "source": [
        "# Appendix D:  Extreme state compression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOE26ol4MQUJ"
      },
      "source": [
        "Taking the approach in Appendix C a step further, I started thinking about whether I could combine the (taxi) empty and full states.  Specifically, as in Hierarchical RL, I wanted to be able to simply have the taxi learn how to get to a point in {R, G, B, Y}, and then re-use that knowledge for both the task of finding the passenger and the task of dropping the passenger off.  As such, I want to compress the state space even further to just 5 x 5 X 4 = 100 states, where I only have one compressed state (meaning a single policy) for each taxi location and destination in {R, G, B, Y}.  \n",
        "\n",
        "As the compressed state now only captures the taxi location and current (possibly temporary) destination, I would ideally like to avoid worrying about pickup and dropoff entirely, since I won't know from the compressed state whether to do a pickup or dropoff (since the compressed state does not capture whether the taxi is empty).  As such, I will always choose a direction as my action, and I will automatically do a pickup or a dropoff (based on whether my decompressed state indicates the taxi is empty or full, respectively) when I arrive at my destination.  Note that to do this, I now need to peek into the API to check whether I have arrived at my current destination; if so, I will force a pickup if the taxi is empty and a dropoff if the taxi is full.  The agent is thus always only choosing from among the 4 directional actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W3E_45uTN_7W",
        "outputId": "be27a405-64f0-426d-eee8-34a1a935741f"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gym\n",
        "\n",
        "\n",
        "# convert (row, col) to 1-base indices and convert pass and dest indices to intuitive strings\n",
        "def pretty_decode(local_taxi, s):\n",
        "    (row, col, pass_idx, dest_idx) = local_taxi.decode(s)\n",
        "    #pretty_row = row+1\n",
        "    #pretty_col = col+1\n",
        "    to_str ={0:'R', 1:'G', 2:'Y', 3:'B', 4:'T'}\n",
        "    return [row+1, col+1, to_str[pass_idx], to_str[dest_idx]]\n",
        "\n",
        "def pretty_decode_compressed(local_taxi, compressed_state):\n",
        "    (row, col, dest_idx) = decode_compressed_state(compressed_state)\n",
        "    to_str_dest = {0:'R', 1:'G', 2:'Y', 3:'B'}\n",
        "    return [row+1, col+1, to_str_dest[dest_idx]]\n",
        "\n",
        "def epsilon_greedy_action_from_Q(Q, state, epsilon):\n",
        "    actions = Q.columns\n",
        "    action_probs = np.asarray([epsilon / len(actions)] * len(actions), dtype=np.float)\n",
        "\n",
        "    greedy_action_index = np.argmax(Q.loc[state].values)\n",
        "    action_probs[greedy_action_index] += 1 - epsilon\n",
        "\n",
        "    epsilon_greedy_action = np.random.choice(Q.columns, p=action_probs)\n",
        "\n",
        "    return epsilon_greedy_action\n",
        "\n",
        "\n",
        "# state compression\n",
        "\n",
        "def encode_compressed_state(taxi_row, taxi_col, dest_idx):\n",
        "    # (5) 5, 2, 4\n",
        "    i = taxi_row\n",
        "    i *= 5\n",
        "    i += taxi_col\n",
        "    i *= 4\n",
        "    i += dest_idx  # passenger loc if passenger not in taxi, else dest loc\n",
        "    return i\n",
        "\n",
        "def decode_compressed_state(i):\n",
        "    dest_idx = i%4\n",
        "    i = i//4\n",
        "    col_idx = i%5\n",
        "    i = i//5\n",
        "    row_idx = i%5\n",
        "    assert 0 <= i < 5\n",
        "    return row_idx, col_idx, dest_idx\n",
        "\n",
        "def compress_state(local_taxi, original_state):\n",
        "    (row, col, pass_idx, dest_idx) = local_taxi.decode(original_state)\n",
        "    if(pass_idx==4):\n",
        "        return encode_compressed_state(row, col, dest_idx)\n",
        "    else:  # not yet in taxi, so destination is irrelevant.  we should use the passenger index as the current destination\n",
        "        return encode_compressed_state(row, col, pass_idx)\n",
        "\n",
        "'''\n",
        "def decompress_state(local_taxi, compressed_state, pass_in_taxi):\n",
        "    (row, col, current_dest_idx) = decode_compressed_state(compressed_state)\n",
        "    if (pass_in_taxi == 1):\n",
        "        return local_taxi.encode(row, col, 4, current_dest_idx)\n",
        "    else:  # not yet in taxi, so destination is irrelevant.  The current destination is the pass_idx.  dest_idx is irrelevant and just needs to not equal pass_idx\n",
        "        return local_taxi.encode(row, col, current_dest_idx, (current_dest_idx+1) %4)\n",
        "'''\n",
        "\n",
        "# Will construct a table where we only care about the destination, which will be pass_loc when pass not in taxi, or dest_idx when pass in taxi\n",
        "# This should allow cross-learning between the 2 phases (pickup and dropoff)\n",
        "# Once reach destination, will automatically do the appropriate pickup or dropoff action, and assign a reward, so only 4 directional actions will typically be considered\n",
        "def q_learning_extreme_compression(taxi, HYPER_PARAMS):\n",
        "\n",
        "    actions_np = np.arange(4) # only 4 actions since agent will only consider directions\n",
        "    states_np = np.arange(5*5*4) # compress to 25*4=100 states by using a single destination which is set to passenger loc when passenger not in taxi and to dest when passenger in taxi\n",
        "\n",
        "    gamma = HYPER_PARAMS['gamma'] # parameter for discounted returns to encourage finding quickest route\n",
        "    n_episodes = HYPER_PARAMS['n_episodes']   # number of episodes to run\n",
        "    epsilon_decay = HYPER_PARAMS['epsilon_decay']   # controls exploration/exploitation tradeoff\n",
        "    min_epsilon = HYPER_PARAMS['min_epsilon']  # controls at what point to stop decaying epsilon (e.g., max exploitation we allow)\n",
        "    alpha = HYPER_PARAMS['alpha'] # effective learning rate telling us how to weight current return against value already in table\n",
        "\n",
        "    max_episode_length = HYPER_PARAMS['max_episode_length'] # max length of episode before forcing a reset in sarsa or q-learning\n",
        "    num_exploration_episodes = HYPER_PARAMS['num_exploration_episodes'] # num episodes to force explore before starting to decay epsilon\n",
        "    num_episodes_with_some_exploration = HYPER_PARAMS['num_episodes_with_some_exploration']\n",
        "\n",
        "    Q = pd.DataFrame.from_dict({s: {a: 0 for a in actions_np} for s in states_np}, orient='index')\n",
        "\n",
        "    epsilon = 1\n",
        "    rewards = np.zeros(n_episodes)\n",
        "    had_illegal_action = np.zeros(n_episodes) # 0 if no illegal action in the episode.  1 if had illegal action\n",
        "\n",
        "    for i in tqdm(range(n_episodes)):\n",
        "        s0 = taxi.reset()\n",
        "        compressed_s0 = compress_state(taxi, s0)\n",
        "        done = False\n",
        "        current_episode_length=0\n",
        "        episode_reward = 0\n",
        "        episode_had_illegal_action = 0\n",
        "\n",
        "        while (not done) and (current_episode_length < max_episode_length):\n",
        "\n",
        "            a0 = epsilon_greedy_action_from_Q(Q, compressed_s0, epsilon) # will be 0->3 since agent will only consider directions\n",
        "            s1, reward, done, dummy_prob = taxi.step(a0)\n",
        "            episode_reward += reward\n",
        "            if reward == -10:\n",
        "                episode_had_illegal_action = 1\n",
        "\n",
        "            compressed_s1 = compress_state(taxi, s1)\n",
        "            Q.loc[compressed_s0, a0] += alpha * (reward + gamma * Q.loc[compressed_s1].max() - Q.loc[compressed_s0, a0])\n",
        "            compressed_s0 = compressed_s1\n",
        "\n",
        "            # check if at destination.  if so, do a pickup or a dropoff\n",
        "            (row_idx, col_idx, pass_loc, dest_idx) = taxi.decode(s1)\n",
        "\n",
        "            if(pass_loc==4): # passenger in taxi\n",
        "                if((row_idx, col_idx)==taxi.locs[dest_idx]): # do a dropoff\n",
        "                    s1, reward, done, dummy_prob = taxi.step(5)\n",
        "                    compressed_s1 = compress_state(taxi, s1)\n",
        "                    episode_reward += reward\n",
        "                    if reward == -10:\n",
        "                        episode_had_illegal_action = 1\n",
        "                    assert done == True  # this better trigger done\n",
        "                    Q.loc[compressed_s0, a0] += alpha * (\n",
        "                                reward + gamma * Q.loc[compressed_s1].max() - Q.loc[compressed_s0, a0])\n",
        "                    compressed_s0 = compressed_s1\n",
        "            else:  # passenger not in taxi\n",
        "                if((row_idx, col_idx)==taxi.locs[pass_loc]): # do a pickup\n",
        "                    s1, reward, done, dummy_prob = taxi.step(4)\n",
        "                    compressed_s1 = compress_state(taxi, s1)\n",
        "                    episode_reward += reward\n",
        "                    if reward == -10:\n",
        "                        episode_had_illegal_action = 1\n",
        "                    Q.loc[compressed_s0, a0] += alpha * (\n",
        "                                reward + gamma * Q.loc[compressed_s1].max() - Q.loc[compressed_s0, a0])\n",
        "                    compressed_s0 = compressed_s1\n",
        "                    # after doing the pickup, compress_state should automatically switch the compressed state's destination to be dest_idx\n",
        "\n",
        "            current_episode_length = current_episode_length+1\n",
        "\n",
        "        if i > num_exploration_episodes:\n",
        "            epsilon *= epsilon_decay\n",
        "            epsilon = max(epsilon, min_epsilon)\n",
        "\n",
        "        # for the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "        # I will therefore force my epsilon to zero after a certain number (num_episodes_with_some_exploration) of episodes.\n",
        "        # Will generally want this to be ~ 1100 less than total num episodes so that get at least 1100 episodes for rubric\n",
        "        if i > num_episodes_with_some_exploration:\n",
        "            epsilon = 0\n",
        "\n",
        "        rewards[i] = episode_reward\n",
        "        had_illegal_action[i] = episode_had_illegal_action\n",
        "\n",
        "    return Q, rewards, had_illegal_action\n",
        "\n",
        "\n",
        "def pretty_print_table(local_taxi, local_table):\n",
        "    new_table = local_table.copy()\n",
        "    num_states = 5*5*4\n",
        "\n",
        "    # Reorder the states in the table such that states with the same passenger and dest indices are grouped together.\n",
        "    n_rows = local_table.shape[0]\n",
        "    stride = np.int32(n_rows/25)\n",
        "\n",
        "    frames = []\n",
        "    for ii in range(stride):\n",
        "        frames.append(new_table[ii::stride]) #.loc(list(np.arange(n_rows)[ii::stride])))\n",
        "        None\n",
        "\n",
        "    reorder_table = pd.concat(frames) #new_table[::stride] #.loc(list(np.arange(n_rows)[::stride]))\n",
        "\n",
        "\n",
        "    # Replace numbered rows with [row, col, pass loc, dest] where row and col are 1-based and pass loc and dest idx are in [RGBY]\n",
        "    pandas_state_pretty_replacer = {}\n",
        "    for s in range(num_states):\n",
        "        pandas_state_pretty_replacer[s] = '['+ ','.join(\n",
        "            [str(x) for x in pretty_decode_compressed(local_taxi, s)])+']'  # tuple(global_taxi.decode(s))\n",
        "\n",
        "    reorder_table.rename(index=pandas_state_pretty_replacer, inplace=True)\n",
        "\n",
        "    # Rename column actions.   Use '+' for pickup and '-' for drop-off\n",
        "    reorder_table.columns = ['S', 'N', 'E', 'W']\n",
        "\n",
        "    print(reorder_table)\n",
        "    return reorder_table\n",
        "\n",
        "def greedy_policy_from_returns_tbl(local_table):\n",
        "    policy = {s: None for s in local_table.index}\n",
        "    for local_state in local_table.index:\n",
        "            greedy_action = local_table.loc[local_state].idxmax()\n",
        "            policy[local_state] = greedy_action\n",
        "    return policy\n",
        "\n",
        "MAP = [\n",
        "    \"+---------+\",\n",
        "    \"|R: | : :G|\",\n",
        "    \"| : | : : |\",\n",
        "    \"| : : : : |\",\n",
        "    \"| | : | : |\",\n",
        "    \"|Y| : |B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "\n",
        "def pretty_print_policy(taxi, local_policy):\n",
        "    direction_repr = {1:' 🡑 ', 2:' 🡒 ', 3:' 🡐 ', 0:' 🡓 '}\n",
        "\n",
        "    # Print policy for each destination.  Policy is same regardless of whether trying to get to passenger or destination\n",
        "    print('Goto Red (top left):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = encode_compressed_state(row, col, 0)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "    print('Goto Green (Top Right):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = encode_compressed_state(row, col, 1)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "    print('Goto Yellow (Bottom Left):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = encode_compressed_state(row, col, 2)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "    print('Goto Blue (Bottom Right):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = encode_compressed_state(row, col, 3)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "\n",
        "\n",
        "# set up hyper-parameters\n",
        "MY_HYPER_PARAMS = {}\n",
        "MY_HYPER_PARAMS['gamma'] = 0.9 # parameter for discounted returns to encourage finding quickest route.\n",
        "MY_HYPER_PARAMS['n_episodes'] = 2000  # number of episodes to run\n",
        "MY_HYPER_PARAMS['epsilon_decay'] = 0.9  # controls exploration/exploitation tradeoff\n",
        "MY_HYPER_PARAMS['min_epsilon'] = 0.05  # controls at what point to stop decaying epsilon (e.g., max exploitation we allow)\n",
        "MY_HYPER_PARAMS['alpha'] = 0.1  # effective learning rate telling us how to weight current return against value already in table\n",
        "MY_HYPER_PARAMS['max_episode_length'] = 50 # max length of episode before forcing a reset in sarsa or q-learning\n",
        "MY_HYPER_PARAMS['num_exploration_episodes'] = 0  # num episodes to force explore before starting to decay epsilon\n",
        "\n",
        "# for the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "# I will therefore force my epsilon to zero after a certain number (num_episodes_with_some_exploration) of episodes.\n",
        "# Will generally want this to be ~ 1100 less than total num episodes so that get at least 1100 episodes for rubric\n",
        "MY_HYPER_PARAMS['num_episodes_with_some_exploration'] = MY_HYPER_PARAMS['n_episodes'] - 1102\n",
        "\n",
        "global_taxi = gym.make('Taxi-v3')\n",
        "\n",
        "estimated_returns_tbl, rewards, had_illegal_action = q_learning_extreme_compression(global_taxi, MY_HYPER_PARAMS)\n",
        "\n",
        "pretty_estimated_returns = pretty_print_table(global_taxi, estimated_returns_tbl)\n",
        "\n",
        "greedy_policy = greedy_policy_from_returns_tbl(estimated_returns_tbl)\n",
        "print(greedy_policy)\n",
        "pretty_print_policy(global_taxi, greedy_policy)\n",
        "\n",
        "windowed_rewards = np.convolve(rewards, np.ones(100)/100, 'valid')\n",
        "plt.plot(windowed_rewards[500:])\n",
        "plt.show()\n",
        "\n",
        "last_1000_start_idx = MY_HYPER_PARAMS['n_episodes'] - 100 - 1000\n",
        "avg_reward_last_1000 = windowed_rewards[last_1000_start_idx:(last_1000_start_idx+999)]\n",
        "\n",
        "avg_reward_5p = np.quantile(avg_reward_last_1000,.05)\n",
        "avg_reward_95p = np.quantile(avg_reward_last_1000,.95)\n",
        "\n",
        "print('5th percentile = '+str(avg_reward_5p))\n",
        "print('95th percentile = '+str(avg_reward_95p))\n",
        "print('Avg reward = '+str(np.mean(avg_reward_last_1000)))\n",
        "\n",
        "illegal_actions = np.count_nonzero(had_illegal_action[last_1000_start_idx:])\n",
        "print('Illegal actions in last 1000 = '+str(illegal_actions))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:28<00:00, 70.88it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                 S          N         E          W\n",
            "[1,1,R]   0.000000  19.761173  0.000000   2.385010\n",
            "[1,2,R]  -0.305531  -0.297010 -0.361000  13.483134\n",
            "[1,3,R]   3.612764  -2.602996 -2.638897  -2.643312\n",
            "[1,4,R]   3.136914  -2.846262 -2.954651  -2.868133\n",
            "[1,5,R]   3.340787  -3.304139 -3.214788  -3.041272\n",
            "...            ...        ...       ...        ...\n",
            "[5,1,B]  -3.753734   5.509521 -3.695630  -3.764746\n",
            "[5,2,B]  -2.449901   4.777026 -2.459400  -2.448417\n",
            "[5,3,B]  -2.299569   6.424202 -2.292497  -2.309413\n",
            "[5,4,B]  20.334750  -0.100000 -0.132944   3.862554\n",
            "[5,5,B]  -0.413445  -0.593693 -0.585199  15.027878\n",
            "\n",
            "[100 rows x 4 columns]\n",
            "{0: 1, 1: 0, 2: 0, 3: 0, 4: 3, 5: 0, 6: 0, 7: 0, 8: 0, 9: 2, 10: 0, 11: 0, 12: 0, 13: 2, 14: 0, 15: 0, 16: 0, 17: 1, 18: 3, 19: 0, 20: 1, 21: 2, 22: 0, 23: 0, 24: 3, 25: 0, 26: 0, 27: 0, 28: 0, 29: 2, 30: 0, 31: 2, 32: 0, 33: 2, 34: 0, 35: 0, 36: 3, 37: 1, 38: 3, 39: 3, 40: 1, 41: 2, 42: 0, 43: 2, 44: 1, 45: 2, 46: 3, 47: 2, 48: 3, 49: 2, 50: 3, 51: 2, 52: 3, 53: 2, 54: 3, 55: 0, 56: 3, 57: 1, 58: 3, 59: 3, 60: 1, 61: 1, 62: 0, 63: 1, 64: 1, 65: 1, 66: 1, 67: 2, 68: 1, 69: 1, 70: 3, 71: 1, 72: 1, 73: 1, 74: 1, 75: 0, 76: 1, 77: 1, 78: 1, 79: 3, 80: 1, 81: 1, 82: 0, 83: 1, 84: 1, 85: 1, 86: 1, 87: 1, 88: 1, 89: 1, 90: 3, 91: 1, 92: 1, 93: 1, 94: 1, 95: 0, 96: 3, 97: 1, 98: 3, 99: 3}\n",
            "Goto Red (top left):\n",
            "| 🡑 : 🡐 | 🡓 : 🡓 : 🡓 \n",
            "| 🡑 : 🡐 | 🡓 : 🡓 : 🡐 \n",
            "| 🡑 : 🡑 : 🡐 : 🡐 : 🡐 \n",
            "| 🡑 | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "| 🡑 | 🡑 : 🡑 | 🡑 : 🡐 \n",
            "Goto Green (Top Right):\n",
            "| 🡓 : 🡓 | 🡒 : 🡒 : 🡑 \n",
            "| 🡒 : 🡓 | 🡒 : 🡒 : 🡑 \n",
            "| 🡒 : 🡒 : 🡒 : 🡒 : 🡑 \n",
            "| 🡑 | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "| 🡑 | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "Goto Yellow (Bottom Left):\n",
            "| 🡓 : 🡓 | 🡓 : 🡓 : 🡐 \n",
            "| 🡓 : 🡓 | 🡓 : 🡓 : 🡐 \n",
            "| 🡓 : 🡐 : 🡐 : 🡐 : 🡐 \n",
            "| 🡓 | 🡑 : 🡐 | 🡑 : 🡑 \n",
            "| 🡓 | 🡑 : 🡐 | 🡑 : 🡐 \n",
            "Goto Blue (Bottom Right):\n",
            "| 🡓 : 🡓 | 🡓 : 🡓 : 🡓 \n",
            "| 🡓 : 🡓 | 🡒 : 🡓 : 🡐 \n",
            "| 🡒 : 🡒 : 🡒 : 🡓 : 🡐 \n",
            "| 🡑 | 🡒 : 🡑 | 🡓 : 🡐 \n",
            "| 🡑 | 🡑 : 🡑 | 🡓 : 🡐 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "5th percentile = 7.61\n",
            "95th percentile = 8.22\n",
            "Avg reward = 7.910990990990991\n",
            "Illegal actions in last 1000 = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4eOlYSrYxkT"
      },
      "source": [
        "As seen, due to the heavily compressed state and action spaces, we can very quickly converge to an excellent policy and meet the rubric.  The only downside is that it required knowledge of whether the taxi is at its temporary destination (the passenger if empty or the final destination if full.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qg-2STjZYzq"
      },
      "source": [
        "# Appendix E:  Dynamic Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9MGht47ZhJA"
      },
      "source": [
        "As another debugging step, I also implemented the dynamic programming approach we discussed towards the beginning of the course.  Although this required knowledge of the environment that is not normally available to the agent, this provided a baseline for an ideal policy and the corresponding rewards that can be reasonably acheivable. Although I implemented this algorithm first when working on the project, I only included it at the very end here to emphasize that the \"cheat\" methods below are never used above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dm0tyEI9aLbR",
        "outputId": "b6d1a125-050d-4fad-a4c1-95474db97b8f"
      },
      "source": [
        "# Use dynamic programming with \"genie\" methods allowing extensive visibility into the environment\n",
        "# Would normally not have this level of visibility, but this is useful for getting a sense of the best acheivable policy and rewards, as a point of comparison\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gym\n",
        "\n",
        "def __is_done__(taxi, local_state, local_action):\n",
        "    return taxi.P[local_state][local_action][0][3] # TODO:  check with Farhan whether this is kosher, or whether I need to explicitly write the code for this\n",
        "\n",
        "def __get_next_state_on_action__(taxi, local_state, local_action):\n",
        "    return taxi.P[local_state][local_action][0][1]\n",
        "\n",
        "def __get_reward_for_action__(taxi, local_state, local_action):\n",
        "    return taxi.P[local_state][local_action][0][2]\n",
        "\n",
        "def compute_returns(taxi, table, state, action, HYPER_PARAMS, debug=True):\n",
        "    \"\"\" Recursively compute the discounted return for a (state,action) pair\"\"\"\n",
        "    if not __is_done__(taxi, state, action):\n",
        "        next_state = __get_next_state_on_action__(taxi, state, action)\n",
        "        reward = __get_reward_for_action__(taxi, state, action)\n",
        "        actions_np = np.arange(taxi.nA)\n",
        "\n",
        "        update = HYPER_PARAMS['gamma'] * \\\n",
        "                 max(table.loc[next_state, actions_np[0]],\n",
        "                     table.loc[next_state, actions_np[1]],\n",
        "                     table.loc[next_state, actions_np[2]],\n",
        "                     table.loc[next_state, actions_np[3]],\n",
        "                     table.loc[next_state, actions_np[4]],\n",
        "                     table.loc[next_state, actions_np[5]],\n",
        "                     )\n",
        "\n",
        "        table.loc[state, action] = reward + update\n",
        "\n",
        "    return table.loc[state, action]\n",
        "\n",
        "def dynamic_programming(taxi, HYPER_PARAMS):\n",
        "    actions_np = np.arange(taxi.nA)\n",
        "    states_np = np.arange(taxi.nS)\n",
        "    RETURNS_TBL = pd.DataFrame.from_dict({s: {a: 0 for a in actions_np} for s in states_np}, orient='index')\n",
        "    for i in range(1, 100):\n",
        "        RETURNS_TBL_OLD = RETURNS_TBL.copy()\n",
        "        for s in states_np:\n",
        "            for a in actions_np:\n",
        "                compute_returns(taxi, RETURNS_TBL, s, a, HYPER_PARAMS) #, debug=True)\n",
        "\n",
        "        deltas = RETURNS_TBL - RETURNS_TBL_OLD\n",
        "        if abs(deltas.values).max() < 1e-3:\n",
        "            print(f'\\nConvergence achieved at {i} iterations')\n",
        "            break\n",
        "    return RETURNS_TBL\n",
        "\n",
        "def epsilon_greedy_action_from_Q(Q, state, epsilon):\n",
        "    actions = Q.columns\n",
        "    action_probs = np.asarray([epsilon / len(actions)] * len(actions), dtype=np.float)\n",
        "\n",
        "    greedy_action_index = np.argmax(Q.loc[state].values)\n",
        "    action_probs[greedy_action_index] += 1 - epsilon\n",
        "\n",
        "    epsilon_greedy_action = np.random.choice(Q.columns, p=action_probs)\n",
        "\n",
        "    return epsilon_greedy_action\n",
        "\n",
        "def get_rewards(taxi, Q, HYPER_PARAMS):\n",
        "\n",
        "    actions_np = np.arange(taxi.nA)\n",
        "    states_np = np.arange(taxi.nS)\n",
        "\n",
        "    gamma = 1 # no discounting\n",
        "    n_episodes = HYPER_PARAMS['n_episodes']   # number of episodes to run\n",
        "    epsilon_decay = HYPER_PARAMS['epsilon_decay']   # controls exploration/exploitation tradeoff\n",
        "    min_epsilon = HYPER_PARAMS['min_epsilon']  # controls at what point to stop decaying epsilon (e.g., max exploitation we allow)\n",
        "    alpha = HYPER_PARAMS['alpha'] # effective learning rate telling us how to weight current return against value already in table\n",
        "\n",
        "    max_episode_length = HYPER_PARAMS['max_episode_length'] # max length of episode before forcing a reset in sarsa or q-learning\n",
        "    num_exploration_episodes = HYPER_PARAMS['num_exploration_episodes'] # num episodes to force explore before starting to decay epsilon\n",
        "    num_episodes_with_some_exploration = HYPER_PARAMS['num_episodes_with_some_exploration']\n",
        "\n",
        "\n",
        "    epsilon = 0 # force greedy actions\n",
        "    rewards = np.zeros(n_episodes)\n",
        "    had_illegal_action = np.zeros(n_episodes) # 0 if no illegal action in the episode.  1 if had illegal action\n",
        "\n",
        "\n",
        "    for i in tqdm(range(n_episodes)):\n",
        "        s0 = taxi.reset()\n",
        "        #s0 = foolsball.init_state\n",
        "        done = False\n",
        "        current_episode_length=0\n",
        "        episode_reward = 0\n",
        "        episode_had_illegal_action = 0\n",
        "\n",
        "        while (not done) and (current_episode_length < max_episode_length):\n",
        "            a0 = epsilon_greedy_action_from_Q(Q, s0, 0)\n",
        "            s1, reward, done, dummy_prob = taxi.step(a0)\n",
        "            # Q.loc[s0, a0] += alpha * (reward + gamma * Q.loc[s1].max() - Q.loc[s0, a0])\n",
        "            s0 = s1\n",
        "            current_episode_length = current_episode_length+1\n",
        "\n",
        "            # For the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "            # I will therefore force my epsilon to zero after a certain number of episodes, after which the below will be based on greedy\n",
        "            # actions instead of epsilon-greedy\n",
        "            episode_reward += reward\n",
        "            if reward == -10:\n",
        "                episode_had_illegal_action = 1\n",
        "\n",
        "        rewards[i] = episode_reward\n",
        "        had_illegal_action[i] = episode_had_illegal_action\n",
        "\n",
        "    return rewards, had_illegal_action\n",
        "\n",
        "\n",
        "    return RETURNS_TBL\n",
        "\n",
        "\n",
        "def greedy_policy_from_returns_tbl(local_table):\n",
        "    policy = {s: None for s in local_table.index}\n",
        "    for local_state in local_table.index:\n",
        "            greedy_action = local_table.loc[local_state].idxmax()\n",
        "            policy[local_state] = greedy_action\n",
        "    return policy\n",
        "\n",
        "def calculate_policy_reward(taxi, local_policy, gamma=1):\n",
        "    local_state = taxi.reset()\n",
        "    taxi.render()\n",
        "    _return_ = 0\n",
        "    discount_coeff = 1\n",
        "    local_act = local_policy[local_state]\n",
        "    max_episode_length = 100 # max length of episode before force quit to avoid loop\n",
        "    episode_length = 0\n",
        "    while episode_length < max_episode_length:\n",
        "        local_state, reward, done, dummy_prob = taxi.step(local_act)\n",
        "        _return_ += discount_coeff * reward\n",
        "        discount_coeff *= gamma\n",
        "        taxi.render()\n",
        "        if done:\n",
        "            print(f'Return (accumulated reward): {_return_}')\n",
        "            return _return_\n",
        "        else:\n",
        "            local_act = local_policy[local_state]\n",
        "            episode_length = episode_length+1\n",
        "\n",
        "    print(f'Reached max_episode_length.  Return (accumulated reward): {_return_}')\n",
        "    return _return_\n",
        "\n",
        "\n",
        "MAP = [\n",
        "    \"+---------+\",\n",
        "    \"|R: | : :G|\",\n",
        "    \"| : | : : |\",\n",
        "    \"| : : : : |\",\n",
        "    \"| | : | : |\",\n",
        "    \"|Y| : |B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "\n",
        "\n",
        "# convert (row, col) to 1-base indices and convert pass and dest indices to intuitive strings\n",
        "def pretty_decode(local_taxi, s):\n",
        "    (row, col, pass_idx, dest_idx) = local_taxi.decode(s)\n",
        "    #pretty_row = row+1\n",
        "    #pretty_col = col+1\n",
        "    to_str ={0:'R', 1:'G', 2:'Y', 3:'B', 4:'T'}\n",
        "    return [row+1, col+1, to_str[pass_idx], to_str[dest_idx]]\n",
        "\n",
        "\n",
        "def pretty_print_table(local_taxi, local_table):\n",
        "    new_table = local_table.copy()\n",
        "\n",
        "    # Group all states with same row,column together\n",
        "    n_rows = local_table.shape[0]\n",
        "    stride = np.int32(n_rows/25)\n",
        "\n",
        "    frames = []\n",
        "    for ii in range(stride):\n",
        "        frames.append(new_table[ii::stride]) #.loc(list(np.arange(n_rows)[ii::stride])))\n",
        "        None\n",
        "\n",
        "    reorder_table = pd.concat(frames) #new_table[::stride] #.loc(list(np.arange(n_rows)[::stride]))\n",
        "\n",
        "\n",
        "    # Replace numbered rows with [row, col, pass loc, dest] where row and col are 1-based and pass loc and dest idx are in [RGBY]\n",
        "    pandas_state_pretty_replacer = {}\n",
        "    for s in range(local_taxi.nS):\n",
        "        pandas_state_pretty_replacer[s] = '['+ ','.join(\n",
        "            [str(x) for x in pretty_decode(local_taxi, s)])+']'  # tuple(global_taxi.decode(s))\n",
        "\n",
        "    reorder_table.rename(index=pandas_state_pretty_replacer, inplace=True)\n",
        "\n",
        "    # Rename column actions.   Use '+' for pickup and '-' for drop-off\n",
        "    reorder_table.columns = ['S', 'N', 'E', 'W', '+', '-']\n",
        "\n",
        "    print(reorder_table)\n",
        "    return reorder_table\n",
        "\n",
        "\n",
        "def pretty_print_policy(taxi, local_policy):\n",
        "    direction_repr = {1:' 🡑 ', 2:' 🡒 ', 3:' 🡐 ', 0:' 🡓 ', 4:' + ', 5:' - ', None:' ⬤ '}\n",
        "\n",
        "\n",
        "    # Print policies for states where we are trying to get to passenger, so dest_idx is irrelevant, as long as not = pass_idx\n",
        "\n",
        "    print('Passenger not in taxi, pass at Red (top left):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 0, 1)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "    print('Passenger not in taxi, pass at Green (Top Right):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 1, 0)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "    print('Passenger not in taxi, pass at yellow (Bottom Left):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 2, 0)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "    print('Passenger not in taxi, pass at Blue (Bottom Right):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 3, 0)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "\n",
        "\n",
        "    # Print policies for states where we already have passenger and are trying to get to destination, so pass_idx is always 4\n",
        "\n",
        "    print('Passenger in taxi, Dest = Red (Top Left):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 4, 0)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "    print('Passenger in taxi, Dest = Green (Top Right):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 4, 1)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "    print('Passenger in taxi, Dest = Yellow (Bottom Left):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 4, 2)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "    print('Passenger in taxi, Dest = Blue (Bottom Right):')\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, 4, 3)\n",
        "            print(MAP[row+1][2*col],end='')\n",
        "            print(direction_repr[local_policy[state]],end='')\n",
        "        print()\n",
        "\n",
        "\n",
        "# set up hyper-parameters\n",
        "MY_HYPER_PARAMS = {}\n",
        "MY_HYPER_PARAMS['gamma'] = 0.9 # parameter for discounted returns to encourage finding quickest route.\n",
        "MY_HYPER_PARAMS['n_episodes'] = 1200  # number of episodes to run\n",
        "MY_HYPER_PARAMS['epsilon_decay'] = 0.99  # controls exploration/exploitation tradeoff\n",
        "MY_HYPER_PARAMS['min_epsilon'] = 0.05  # controls at what point to stop decaying epsilon (e.g., max exploitation we allow)\n",
        "MY_HYPER_PARAMS['alpha'] = 0.1  # effective learning rate telling us how to weight current return against value already in table\n",
        "MY_HYPER_PARAMS['max_episode_length'] = 50 # max length of episode before forcing a reset in sarsa or q-learning\n",
        "MY_HYPER_PARAMS['num_exploration_episodes'] = 500  # num episodes to force explore before starting to decay epsilon\n",
        "\n",
        "# for the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "# I will therefore force my epsilon to zero after a certain number (num_episodes_with_some_exploration) of episodes.\n",
        "# Will generally want this to be ~ 1100 less than total num episodes so that get at least 1100 episodes for rubric\n",
        "MY_HYPER_PARAMS['num_episodes_with_some_exploration'] = MY_HYPER_PARAMS['n_episodes'] - 1102\n",
        "\n",
        "global_taxi = gym.make('Taxi-v3')\n",
        "\n",
        "estimated_returns_tbl = dynamic_programming(global_taxi, MY_HYPER_PARAMS) #dynamic_programming(taxi, MY_HYPER_PARAMS) # ESTIMATED_RETURNS_TBL\n",
        "\n",
        "pretty_estimated_returns = pretty_print_table(global_taxi, estimated_returns_tbl)\n",
        "\n",
        "\n",
        "greedy_policy = greedy_policy_from_returns_tbl(estimated_returns_tbl)\n",
        "print(greedy_policy)\n",
        "\n",
        "pretty_print_policy(global_taxi, greedy_policy)\n",
        "\n",
        "#random_initial_state = taxi.reset()\n",
        "#policy_reward = calculate_policy_reward(global_taxi, q_policy)\n",
        "\n",
        "rewards, had_illegal_action = get_rewards(global_taxi, estimated_returns_tbl, MY_HYPER_PARAMS)\n",
        "\n",
        "windowed_rewards = np.convolve(rewards, np.ones(100)/100, 'valid')\n",
        "plt.plot(windowed_rewards[500:])\n",
        "plt.show()\n",
        "\n",
        "last_1000_start_idx = MY_HYPER_PARAMS['n_episodes'] - 100 - 1000\n",
        "avg_reward_last_1000 = windowed_rewards[last_1000_start_idx:(last_1000_start_idx+999)]\n",
        "\n",
        "avg_reward_5p = np.quantile(avg_reward_last_1000,.05)\n",
        "avg_reward_95p = np.quantile(avg_reward_last_1000,.95)\n",
        "\n",
        "print('5th percentile = '+str(avg_reward_5p))\n",
        "print('95th percentile = '+str(avg_reward_95p))\n",
        "print('Avg reward = '+str(np.mean(avg_reward_last_1000)))\n",
        "\n",
        "illegal_actions = np.count_nonzero(had_illegal_action[last_1000_start_idx:])\n",
        "print('Illegal actions in last 1000 = '+str(illegal_actions))\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 23/1200 [00:00<00:05, 227.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Convergence achieved at 19 iterations\n",
            "                  S         N         E         W          +          -\n",
            "[1,1,R,R] -2.710000 -1.900000 -2.710000 -1.900000  -1.000000 -10.900000\n",
            "[1,2,R,R] -3.439000 -2.710000 -2.710000 -1.900000 -11.710000 -11.710000\n",
            "[1,3,R,R] -5.217031 -5.695328 -6.125795 -5.695328 -14.695328 -14.695328\n",
            "[1,4,R,R] -5.695328 -6.125795 -6.513216 -5.695328 -15.125795 -15.125795\n",
            "[1,5,R,R] -6.125795 -6.513216 -6.513216 -6.125795 -15.513216 -15.513216\n",
            "...             ...       ...       ...       ...        ...        ...\n",
            "[5,1,T,B] -5.695328 -5.217031 -5.695328 -5.695328 -14.695328  -6.125795\n",
            "[5,2,T,B] -5.217031 -4.685590 -4.685590 -5.217031 -14.217031 -14.217031\n",
            "[5,3,T,B] -4.685590 -4.095100 -4.685590 -5.217031 -13.685590 -13.685590\n",
            "[5,4,T,B] -1.000000 -1.900000 -1.900000 -1.000000 -10.000000   0.000000\n",
            "[5,5,T,B] -1.900000 -2.710000 -1.900000 -1.000000 -10.900000 -10.900000\n",
            "\n",
            "[500 rows x 6 columns]\n",
            "{0: 4, 1: 4, 2: 4, 3: 4, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 5, 17: 0, 18: 0, 19: 0, 20: 3, 21: 3, 22: 3, 23: 3, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 3, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 2, 45: 2, 46: 2, 47: 2, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 2, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 2, 65: 2, 66: 2, 67: 2, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 2, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 4, 85: 4, 86: 4, 87: 4, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 5, 98: 0, 99: 0, 100: 1, 101: 1, 102: 1, 103: 1, 104: 0, 105: 0, 106: 0, 107: 0, 108: 0, 109: 0, 110: 0, 111: 0, 112: 0, 113: 0, 114: 0, 115: 0, 116: 1, 117: 0, 118: 0, 119: 0, 120: 1, 121: 1, 122: 1, 123: 1, 124: 0, 125: 0, 126: 0, 127: 0, 128: 0, 129: 0, 130: 0, 131: 0, 132: 0, 133: 0, 134: 0, 135: 0, 136: 1, 137: 0, 138: 0, 139: 0, 140: 0, 141: 0, 142: 0, 143: 0, 144: 1, 145: 1, 146: 1, 147: 1, 148: 0, 149: 0, 150: 0, 151: 0, 152: 0, 153: 0, 154: 0, 155: 0, 156: 0, 157: 1, 158: 0, 159: 0, 160: 0, 161: 0, 162: 0, 163: 0, 164: 1, 165: 1, 166: 1, 167: 1, 168: 0, 169: 0, 170: 0, 171: 0, 172: 0, 173: 0, 174: 0, 175: 0, 176: 0, 177: 1, 178: 0, 179: 0, 180: 0, 181: 0, 182: 0, 183: 0, 184: 1, 185: 1, 186: 1, 187: 1, 188: 0, 189: 0, 190: 0, 191: 0, 192: 0, 193: 0, 194: 0, 195: 0, 196: 0, 197: 1, 198: 0, 199: 0, 200: 1, 201: 1, 202: 1, 203: 1, 204: 2, 205: 2, 206: 2, 207: 2, 208: 0, 209: 0, 210: 0, 211: 0, 212: 2, 213: 2, 214: 2, 215: 2, 216: 1, 217: 2, 218: 0, 219: 2, 220: 1, 221: 1, 222: 1, 223: 1, 224: 2, 225: 2, 226: 2, 227: 2, 228: 3, 229: 3, 230: 3, 231: 3, 232: 2, 233: 2, 234: 2, 235: 2, 236: 1, 237: 2, 238: 3, 239: 2, 240: 3, 241: 3, 242: 3, 243: 3, 244: 1, 245: 1, 246: 1, 247: 1, 248: 3, 249: 3, 250: 3, 251: 3, 252: 2, 253: 2, 254: 2, 255: 2, 256: 3, 257: 1, 258: 3, 259: 2, 260: 3, 261: 3, 262: 3, 263: 3, 264: 1, 265: 1, 266: 1, 267: 1, 268: 3, 269: 3, 270: 3, 271: 3, 272: 0, 273: 0, 274: 0, 275: 0, 276: 3, 277: 1, 278: 3, 279: 0, 280: 3, 281: 3, 282: 3, 283: 3, 284: 1, 285: 1, 286: 1, 287: 1, 288: 3, 289: 3, 290: 3, 291: 3, 292: 0, 293: 0, 294: 0, 295: 0, 296: 3, 297: 1, 298: 3, 299: 0, 300: 1, 301: 1, 302: 1, 303: 1, 304: 1, 305: 1, 306: 1, 307: 1, 308: 0, 309: 0, 310: 0, 311: 0, 312: 1, 313: 1, 314: 1, 315: 1, 316: 1, 317: 1, 318: 0, 319: 1, 320: 1, 321: 1, 322: 1, 323: 1, 324: 1, 325: 1, 326: 1, 327: 1, 328: 1, 329: 1, 330: 1, 331: 1, 332: 1, 333: 1, 334: 1, 335: 1, 336: 1, 337: 1, 338: 1, 339: 1, 340: 1, 341: 1, 342: 1, 343: 1, 344: 1, 345: 1, 346: 1, 347: 1, 348: 1, 349: 1, 350: 1, 351: 1, 352: 1, 353: 1, 354: 1, 355: 1, 356: 1, 357: 1, 358: 1, 359: 1, 360: 1, 361: 1, 362: 1, 363: 1, 364: 1, 365: 1, 366: 1, 367: 1, 368: 1, 369: 1, 370: 1, 371: 1, 372: 0, 373: 0, 374: 0, 375: 0, 376: 1, 377: 1, 378: 1, 379: 0, 380: 1, 381: 1, 382: 1, 383: 1, 384: 1, 385: 1, 386: 1, 387: 1, 388: 1, 389: 1, 390: 1, 391: 1, 392: 0, 393: 0, 394: 0, 395: 0, 396: 1, 397: 1, 398: 1, 399: 0, 400: 1, 401: 1, 402: 1, 403: 1, 404: 1, 405: 1, 406: 1, 407: 1, 408: 4, 409: 4, 410: 4, 411: 4, 412: 1, 413: 1, 414: 1, 415: 1, 416: 1, 417: 1, 418: 5, 419: 1, 420: 1, 421: 1, 422: 1, 423: 1, 424: 1, 425: 1, 426: 1, 427: 1, 428: 1, 429: 1, 430: 1, 431: 1, 432: 1, 433: 1, 434: 1, 435: 1, 436: 1, 437: 1, 438: 1, 439: 1, 440: 1, 441: 1, 442: 1, 443: 1, 444: 1, 445: 1, 446: 1, 447: 1, 448: 1, 449: 1, 450: 1, 451: 1, 452: 1, 453: 1, 454: 1, 455: 1, 456: 1, 457: 1, 458: 1, 459: 1, 460: 1, 461: 1, 462: 1, 463: 1, 464: 1, 465: 1, 466: 1, 467: 1, 468: 1, 469: 1, 470: 1, 471: 1, 472: 4, 473: 4, 474: 4, 475: 4, 476: 1, 477: 1, 478: 1, 479: 5, 480: 1, 481: 1, 482: 1, 483: 1, 484: 1, 485: 1, 486: 1, 487: 1, 488: 1, 489: 1, 490: 1, 491: 1, 492: 3, 493: 3, 494: 3, 495: 3, 496: 1, 497: 1, 498: 1, 499: 3}\n",
            "Passenger not in taxi, pass at Red (top left):\n",
            "| + : 🡐 | 🡓 : 🡓 : 🡓 \n",
            "| 🡑 : 🡑 | 🡓 : 🡓 : 🡓 \n",
            "| 🡑 : 🡑 : 🡐 : 🡐 : 🡐 \n",
            "| 🡑 | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "| 🡑 | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "Passenger not in taxi, pass at Green (Top Right):\n",
            "| 🡓 : 🡓 | 🡒 : 🡒 : + \n",
            "| 🡓 : 🡓 | 🡑 : 🡑 : 🡑 \n",
            "| 🡒 : 🡒 : 🡑 : 🡑 : 🡑 \n",
            "| 🡑 | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "| 🡑 | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "Passenger not in taxi, pass at yellow (Bottom Left):\n",
            "| 🡓 : 🡓 | 🡓 : 🡓 : 🡓 \n",
            "| 🡓 : 🡓 | 🡓 : 🡓 : 🡓 \n",
            "| 🡓 : 🡐 : 🡐 : 🡐 : 🡐 \n",
            "| 🡓 | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "| + | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "Passenger not in taxi, pass at Blue (Bottom Right):\n",
            "| 🡓 : 🡓 | 🡓 : 🡓 : 🡓 \n",
            "| 🡓 : 🡓 | 🡓 : 🡓 : 🡓 \n",
            "| 🡒 : 🡒 : 🡒 : 🡓 : 🡓 \n",
            "| 🡑 | 🡑 : 🡑 | 🡓 : 🡓 \n",
            "| 🡑 | 🡑 : 🡑 | + : 🡐 \n",
            "Passenger in taxi, Dest = Red (Top Left):\n",
            "| - : 🡐 | 🡓 : 🡓 : 🡓 \n",
            "| 🡑 : 🡑 | 🡓 : 🡓 : 🡓 \n",
            "| 🡑 : 🡑 : 🡐 : 🡐 : 🡐 \n",
            "| 🡑 | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "| 🡑 | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "Passenger in taxi, Dest = Green (Top Right):\n",
            "| 🡓 : 🡓 | 🡒 : 🡒 : - \n",
            "| 🡓 : 🡓 | 🡑 : 🡑 : 🡑 \n",
            "| 🡒 : 🡒 : 🡑 : 🡑 : 🡑 \n",
            "| 🡑 | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "| 🡑 | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "Passenger in taxi, Dest = Yellow (Bottom Left):\n",
            "| 🡓 : 🡓 | 🡓 : 🡓 : 🡓 \n",
            "| 🡓 : 🡓 | 🡓 : 🡓 : 🡓 \n",
            "| 🡓 : 🡐 : 🡐 : 🡐 : 🡐 \n",
            "| 🡓 | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "| - | 🡑 : 🡑 | 🡑 : 🡑 \n",
            "Passenger in taxi, Dest = Blue (Bottom Right):\n",
            "| 🡓 : 🡓 | 🡓 : 🡓 : 🡓 \n",
            "| 🡓 : 🡓 | 🡓 : 🡓 : 🡓 \n",
            "| 🡒 : 🡒 : 🡒 : 🡓 : 🡓 \n",
            "| 🡑 | 🡑 : 🡑 | 🡓 : 🡓 \n",
            "| 🡑 | 🡑 : 🡑 | - : 🡐 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [00:05<00:00, 205.32it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "5th percentile = 7.588999999999999\n",
            "95th percentile = 8.36\n",
            "Avg reward = 7.967927927927928\n",
            "Illegal actions in last 1000 = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrRkr6sganTb"
      },
      "source": [
        "I was thus able to acheive rewards on par with the dynamic programming \"genie\"-based approach, even with imperfect knowledge of the environment, by using, for example, the Q-learning approach."
      ]
    }
  ]
}