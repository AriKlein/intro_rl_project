{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNfzCfC5h+/1XmYCTdNEUzo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AriKlein/intro_rl_project/blob/main/intro_rl_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGeLz_REsKzj"
      },
      "source": [
        "# Introduction to Reinforcement Learning Final Project\n",
        "# Ari Klein (aeklein)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7iVz0l4GCCb"
      },
      "source": [
        "# 1)  Describe the methods and variables in the class DiscreteEnv which is the parent class of the Taxi V3 class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGNPxYW3GU9W"
      },
      "source": [
        "DiscreteEnv has member data:\n",
        "-\tnS and nA capturing the number of states and actions, respectively.  \n",
        "-\tisd, which is a list of length nS giving the probability, for each state, that the initial state gets set to that state for any given call to reset()\n",
        "-\tP, which is a dictionary storing, for each {state, action} pair, the corresponding next state, the corresponding reward, and whether, after taking that action from the initial state, the system is now in a terminal state.  If the transitions are probabilistic, it stores all possible sets of {next_state, reward, done} in a list along with the probability associated with each possible {next_state, reward, done}\n",
        "-\tlastaction, which stores the previous action taken\n",
        "-\ts, which stores the current state\n",
        "-\taction_space and observation_space, which are discrete spaces of size nA and nS, respectively, representing the space of possible actions and states, respectively. \n",
        "DiscreteEnv has member functions (e.g. methods):\n",
        "-\tinit for constructing a DiscreteEnv object with member data set to the values specified in its arguments\n",
        "-\tseed for setting the seed for the RNG to a specified value or to a random one if no value is specified\n",
        "-\treset, which resets the system by setting the state to an initial state sampled from the set of possible states according to distribution isd.  Also sets lastaction to None\n",
        "-\tstep, which takes an action, moves from the current state to the next state based on that action, and returns the next state, associated reward, and whether the system is now in a terminal state.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee0Tk3iaGnd2"
      },
      "source": [
        "DiscreteEnv has member functions (e.g. methods):\n",
        "-\tinit for constructing a DiscreteEnv object with member data set to the values specified in its arguments\n",
        "-\tseed for setting the seed for the RNG to a specified value or to a random one if no value is specified\n",
        "-\treset, which resets the system by setting the state to an initial state sampled from the set of possible states according to distribution isd.  Also sets lastaction to None\n",
        "-\tstep, which takes an action, moves from the current state to the next state based on that action, and returns the next state, associated reward, and whether the system is now in a terminal state.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94Rk2s8PriaG"
      },
      "source": [
        "# 2) Describe the methods and variables in the Taxi V3 class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2id-ybYTpya_"
      },
      "source": [
        "As TaxiEnv inherits DiscreteEnv, it has all the data and functions described above:\n",
        "-\tThe state captures the location on the map (out of 25 possible locations), the passenger location (5 total, including the possibility that the passenger is in the taxi), and the destination locations (out of 4 possible), for a total of nS = 25 X 5 X 4 = 500 possible states.\n",
        "-\tFrom each state, it is possible to move in any of 4 directions, or do a pickup, or do a dropoff, for a total of nA=6 possible actions.\n",
        "-\tThe initial state is a randomly chosen state from the 500 (= 5 X 5 X 5 X 4) possible states, subject to the constraint that the (initial) passenger location cannot be equal to the destination location (ruling out 5 X 5 X 4=100 possible states), nor can the passenger initially be inside the taxi, ruling out passenger location index 4, corresponding to 5 X 5 X 4 = 100 possible states.  This leaves 300 valid possible initial states, with the initial state being equally likely (e.g., with probability of 1/300) to be any one of these valid states.  isd is thus set to 1/300 for each of the 300 valid possible initial states, and 0 for the invalid initial states.\n",
        "-\tThe P dictionary captures the set of possible next states resulting from taking any action in any state, along with their associated probabilities, rewards, and whether they are terminal states.  Since the taxi environment is fully deterministic, there is only a single possible next state resulting from any action in any state, and it is assigned a probability of 1.  The only scenario resulting in a terminal state is performing a drop-off when the passenger is in the taxi and the taxi is at the correct destination.  Almost all actions in all states are assigned a reward of -1, with 3 exceptions:\n",
        " - Attempting a pickup when the passenger is either already in the taxi or not at the same location as the taxi is an illegal pickup and results in a reward of -10.\n",
        " - Attempting a drop-off when the passenger is either not in the taxi, or when the taxi is not in one of the four marked locations is an illegal drop-off and results in a reward of -10.\n",
        " - Performing a drop-off when the passenger is in the taxi and the taxi is at the correct destination, which results in a reward of 20.\n",
        "\n",
        "The new member data which are unique to the Taxi environment (e.g., not inherited from DiscreteEnv) are:\n",
        "-\tdesc, which captures the text-based map of the environment\n",
        "-\tlocs, which captures the possible pickup and dropoff locations as an array of ordered pairs on a 5X5 grid.\n",
        "\n",
        "The member functions are as follows:\n",
        "-\tinit sets the member data as described above.  \n",
        "-\treset resets the state of the environment to a random (uniformly distributed) state from the set of valid initial states, meaning that the initial taxi location, passenger location, and destination, are randomly chosen upon any call to reset.\n",
        "-\tstep takes an action, moves from the current state to the next state based on that action, and returns the next state, associated reward, and whether the system is now in a terminal state.  \n",
        "-\tencode converts a tuple containing the taxi row (0 to 4 going from north to south), taxi column (0 to 4 going from west to east), passenger location (index in locs, or 4 to indicate the passenger is in the taxi), and desired destination, and converts this tuple to a single number in [0,499].  In other words, it converts a logical representation of the state to a single integer representing the state.\n",
        "-\tdecode does the inverse of encode, namely converting a single integer representing the state into the corresponding taxi row, taxi column, passenger location and desired destination.\n",
        "-\trender depicts the environment and current state in an intuitive, human-readable, text-based format, where the taxi is shown on the map in its current location, with the taxi colored green if it has already successfully picked up the passenger, and yellow otherwise.  The location with the passenger (pre-pickup) is colored blue, while the desired destination is colored magenta.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhf7Lu7gryI2"
      },
      "source": [
        "# 3) Describe the Taxi V3 environment, its actions, states, reward structure and the rationale behind such a reward structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca-nWkQIsxPC"
      },
      "source": [
        "The Taxi V3 environment consists of a taxi agent which is initially at some random location on the below map:\n",
        "\n",
        "```\n",
        "MAP = [\n",
        "    \"+---------+\",\n",
        "    \"|R: | : :G|\",\n",
        "    \"| : | : : |\",\n",
        "    \"| : : : : |\",\n",
        "    \"| | : | : |\",\n",
        "    \"|Y| : |B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "```\n",
        "where the above map is illustrating a 5X5 grid.  The taxi can try to move north, east, west, or south, but will only be able to successfully move east or west through a `':'`.  There is a passenger randomly placed in the environment at one of the four location marked R, G, B, or Y, with a (different) desired destination that is randomly either R, G, B, or Y.  The goal of the taxi agent is to:\n",
        " - move to the passenger location\n",
        " - pick up the passenger at the passenger location, at which point the passenger is inside the taxi\n",
        " - move to the destination location\n",
        " - drop off the passenger at the destination location\n",
        "\n",
        "The taxi agent can always attempt to take one of 6 possible actions:\n",
        "- move south, north, east or west (actions 0-3, respectively)\n",
        "- pick up a passenger (action 4)\n",
        "- drop off a passenger (action 5)\n",
        "\n",
        "The state captures the location of the taxi agent on the map (out of 25 possible locations), the passenger location (5 total, including the possibility that the passenger is in the taxi), and the destination locations (out of 4 possible), for a total of 25 X 5 X 4 = 500 possible states.\n",
        "\n",
        "Almost all actions in all states are assigned a reward of -1, with 3 exceptions:\n",
        "- Attempting a pickup when the passenger is either already in the taxi or not at the same location as the taxi is an illegal pickup and results in a reward of -10.\n",
        "- Attempting a drop-off when the passenger is either not in the taxi, or when the taxi is not in one of the four marked locations is an illegal drop-off and results in a reward of -10.\n",
        "- Performing a drop-off when the passenger is in the taxi and the taxi is at the correct destination, which results in a reward of 20.\n",
        "\n",
        "The reward for successful completion of the final objective is to encourage the agent to successfully complete the objective; this reward needs to be effectively \"propagated\" backwards to encourage actions which lead the agent closer to acheiving the objective.  The rationale behind the default reward of -1 for almost all steps is to encourage the agent to acheive the goals described above in as few steps as possible (e.g., to encourage efficiency).  The relatively large penalties for illegal pickups and dropoffs are to strongly discourage the agent from taking such actions.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMk9aQY5Rb1t"
      },
      "source": [
        "# 4-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z37exgbBRtTn"
      },
      "source": [
        "Below are my implementations of SARSA and Q-learning, which both call my implementation of epsilon_greedy_action_from_Q.  The code for these three methods is largely based off the W3S2 and W4S1 solution branches (for example, https://github.com/KnowchowHQ/rl-in-action/blob/solution/C1-RL-Intro/W4S1/foolsball-v3.ipynb) which is in turn based off https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#sarsa-on-policy-td-control. \n",
        "My main changes to this code were:\n",
        " - The addition of hyperparameter `max_episode_length` for forcing episode end if taxi is looping\n",
        " - The addition of hyperparameter `num_exploration_episodes` to force exploration (epsilon=1) for several episodes before starting exploitation (epsilon decay)\n",
        " - The addition of hyperparameter `num_episodes_with_some_exploration` for forcing epsilon to 0 to force greedy policy for the purpose of calculating rubric metrics.\n",
        " - Calculating and returning the accumulated, non-discounted reward for each episode and checking whether each episode contains any illegal actions.  These are needed to evaluate algorithm performance against the rubric.  Since we should always be taking greedy actions fot the rubric, epsilon should be set to 0 for at least the last 1000 episodes (or 1100 to get 1000 sliding windows of length 100)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq3nP3w--Hbn"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gym\n",
        "\n",
        "# Given:\n",
        "# - a pandas table, Q, of discounted returns for for each action (columns) from each state (rows)\n",
        "# - a state\n",
        "# - an epsilon value\n",
        "# Returns an action sampled from a probability distribution wherein:\n",
        "# - The greedy action is taken with probability (1-epsilon) + epsilon/nA  (nA is number of possible actions)\n",
        "# - Any single non-greedy action can be taken with probability epsilon/nA  (nA is number of possible actions)\n",
        "# When epsilon=0, the below always gives the greedy action\n",
        "# When epsilon=1, the action is completely random (e.g., sampled from a uniform dist. across the possible actions)\n",
        "def epsilon_greedy_action_from_Q(Q, state, epsilon):\n",
        "    actions = Q.columns\n",
        "    action_probs = np.asarray([epsilon / len(actions)] * len(actions), dtype=np.float)\n",
        "\n",
        "    greedy_action_index = np.argmax(Q.loc[state].values)\n",
        "    action_probs[greedy_action_index] += 1 - epsilon\n",
        "\n",
        "    epsilon_greedy_action = np.random.choice(Q.columns, p=action_probs)\n",
        "\n",
        "    return epsilon_greedy_action\n",
        "\n",
        "# Implements sarsa, given a taxi agent and a dictionary of hyperparameters\n",
        "# Returns:\n",
        "#   - Pandas table of discounted returns for for each action (columns) from each state (rows)\n",
        "#   - numpy array of length n_episodes with reward for each epsiode.  Generally non-dsicounted but using epsilon-greedy policy.  By forcing epsilon to 0 for the last 1100 episodes, can get greedy rewards.\n",
        "#   - numpy array of length n_episodes with a '1' if the episode had at least 1 illegal action (action resulting in reward of -10), and a '0' if no illegal action was taken in the epsiode.\n",
        "def sarsa(taxi, HYPER_PARAMS):\n",
        "\n",
        "    actions_np = np.arange(taxi.nA)\n",
        "    states_np = np.arange(taxi.nS)\n",
        "\n",
        "    gamma = HYPER_PARAMS['gamma'] # parameter for discounted returns to encourage finding quickest route\n",
        "    n_episodes = HYPER_PARAMS['n_episodes']   # number of episodes to run\n",
        "    epsilon_decay = HYPER_PARAMS['epsilon_decay']   # controls exploration/exploitation tradeoff\n",
        "    min_epsilon = HYPER_PARAMS['min_epsilon']  # controls at what point to stop decaying epsilon (e.g., max exploitation we allow)\n",
        "    alpha = HYPER_PARAMS['alpha'] # effective learning rate telling us how to weight current return against value already in table\n",
        "\n",
        "    max_episode_length = HYPER_PARAMS['max_episode_length'] # max length of episode before forcing a reset in sarsa or q-learning\n",
        "    num_exploration_episodes = HYPER_PARAMS['num_exploration_episodes'] # num episodes to force explore before starting to decay epsilon\n",
        "    num_episodes_with_some_exploration = HYPER_PARAMS['num_episodes_with_some_exploration']\n",
        "\n",
        "    Q = pd.DataFrame.from_dict({s: {a: 0 for a in actions_np} for s in states_np}, orient='index')\n",
        "\n",
        "    epsilon = 1\n",
        "    rewards = np.zeros(n_episodes)\n",
        "    had_illegal_action = np.zeros(n_episodes) # 0 if no illegal action in the episode.  1 if had illegal action\n",
        "\n",
        "\n",
        "    for i in tqdm(range(n_episodes)):\n",
        "        s0 = taxi.reset()\n",
        "        a0 = epsilon_greedy_action_from_Q(Q, s0, epsilon)\n",
        "        #s0 = foolsball.init_state\n",
        "        done = False\n",
        "        current_episode_length=0\n",
        "        episode_reward = 0\n",
        "        episode_had_illegal_action = 0\n",
        "\n",
        "        while (not done) and (current_episode_length < max_episode_length):\n",
        "\n",
        "            s1, reward, done, dummy_prob = taxi.step(a0)\n",
        "            a1 = epsilon_greedy_action_from_Q(Q, s1, epsilon)\n",
        "            Q.loc[s0, a0] += alpha * (reward + gamma * Q.loc[s1, a1] - Q.loc[s0, a0])\n",
        "            s0 = s1\n",
        "            a0 = a1\n",
        "            current_episode_length = current_episode_length+1\n",
        "\n",
        "            # For the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "            # I will therefore force my epsilon to zero after a certain number of episodes, after which the below will be based on greedy\n",
        "            # actions instead of epsilon-greedy\n",
        "            episode_reward += reward\n",
        "            if reward == -10:\n",
        "                episode_had_illegal_action = 1\n",
        "\n",
        "        if i > num_exploration_episodes:\n",
        "            epsilon *= epsilon_decay\n",
        "            epsilon = max(epsilon, min_epsilon)\n",
        "\n",
        "        # for the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "        # I will therefore force my epsilon to zero after a certain number (num_episodes_with_some_exploration) of episodes.\n",
        "        # Will generally want this to be ~ 1100 less than total num episodes so that get at least 1100 episodes for rubric\n",
        "        if i > num_episodes_with_some_exploration:\n",
        "            epsilon = 0\n",
        "\n",
        "        rewards[i] = episode_reward\n",
        "        had_illegal_action[i] = episode_had_illegal_action\n",
        "\n",
        "    return Q, rewards, had_illegal_action\n",
        "\n",
        "# Implements Q-learning, given a taxi agent and a dictionary of hyperparameters\n",
        "# Returns:\n",
        "#   - Pandas table of discounted returns for for each action (columns) from each state (rows)\n",
        "#   - numpy array of length n_episodes with reward for each epsiode.  Generally non-dsicounted but using epsilon-greedy policy.  By forcing epsilon to 0 for the last 1100 episodes, can get greedy rewards.\n",
        "#   - numpy array of length n_episodes with a '1' if the episode had at least 1 illegal action (action resulting in reward of -10), and a '0' if no illegal action was taken in the epsiode.\n",
        "def q_learning(taxi, HYPER_PARAMS):\n",
        "\n",
        "    actions_np = np.arange(taxi.nA)\n",
        "    states_np = np.arange(taxi.nS)\n",
        "\n",
        "    gamma = HYPER_PARAMS['gamma'] # parameter for discounted returns to encourage finding quickest route\n",
        "    n_episodes = HYPER_PARAMS['n_episodes']   # number of episodes to run\n",
        "    epsilon_decay = HYPER_PARAMS['epsilon_decay']   # controls exploration/exploitation tradeoff\n",
        "    min_epsilon = HYPER_PARAMS['min_epsilon']  # controls at what point to stop decaying epsilon (e.g., max exploitation we allow)\n",
        "    alpha = HYPER_PARAMS['alpha'] # effective learning rate telling us how to weight current return against value already in table\n",
        "\n",
        "    max_episode_length = HYPER_PARAMS['max_episode_length'] # max length of episode before forcing a reset in sarsa or q-learning\n",
        "    num_exploration_episodes = HYPER_PARAMS['num_exploration_episodes'] # num episodes to force explore before starting to decay epsilon\n",
        "    num_episodes_with_some_exploration = HYPER_PARAMS['num_episodes_with_some_exploration']\n",
        "\n",
        "    Q = pd.DataFrame.from_dict({s: {a: 0 for a in actions_np} for s in states_np}, orient='index')\n",
        "\n",
        "    epsilon = 1\n",
        "    rewards = np.zeros(n_episodes)\n",
        "    had_illegal_action = np.zeros(n_episodes) # 0 if no illegal action in the episode.  1 if had illegal action\n",
        "\n",
        "\n",
        "    for i in tqdm(range(n_episodes)):\n",
        "        s0 = taxi.reset()\n",
        "        #s0 = foolsball.init_state\n",
        "        done = False\n",
        "        current_episode_length=0\n",
        "        episode_reward = 0\n",
        "        episode_had_illegal_action = 0\n",
        "\n",
        "        while (not done) and (current_episode_length < max_episode_length):\n",
        "            a0 = epsilon_greedy_action_from_Q(Q, s0, epsilon)\n",
        "            s1, reward, done, dummy_prob = taxi.step(a0)\n",
        "            Q.loc[s0, a0] += alpha * (reward + gamma * Q.loc[s1].max() - Q.loc[s0, a0])\n",
        "            s0 = s1\n",
        "            current_episode_length = current_episode_length+1\n",
        "\n",
        "            # For the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "            # I will therefore force my epsilon to zero after a certain number of episodes, after which the below will be based on greedy\n",
        "            # actions instead of epsilon-greedy\n",
        "            episode_reward += reward\n",
        "            if reward == -10:\n",
        "                episode_had_illegal_action = 1\n",
        "\n",
        "        if i > num_exploration_episodes:\n",
        "            epsilon *= epsilon_decay\n",
        "            epsilon = max(epsilon, min_epsilon)\n",
        "\n",
        "        # for the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "        # I will therefore force my epsilon to zero after a certain number (num_episodes_with_some_exploration) of episodes.\n",
        "        # Will generally want this to be ~ 1100 less than total num episodes so that get at least 1100 episodes for rubric\n",
        "        if i > num_episodes_with_some_exploration:\n",
        "            epsilon = 0\n",
        "\n",
        "        rewards[i] = episode_reward\n",
        "        had_illegal_action[i] = episode_had_illegal_action\n",
        "\n",
        "    return Q, rewards, had_illegal_action"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvYp9pqBX0gm"
      },
      "source": [
        "I defined the following 8 hyperparameters (description of each given in code comments):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21UnOhyKYLsG"
      },
      "source": [
        "MY_HYPER_PARAMS = {}\n",
        "MY_HYPER_PARAMS['gamma'] = 0.9 # parameter for discounted returns to encourage finding quickest route.\n",
        "MY_HYPER_PARAMS['n_episodes'] = 10000  # number of episodes to run\n",
        "MY_HYPER_PARAMS['epsilon_decay'] = 0.99  # controls exploration/exploitation tradeoff\n",
        "MY_HYPER_PARAMS['min_epsilon'] = 0.05  # controls at what point to stop decaying epsilon (e.g., max exploitation we allow)\n",
        "MY_HYPER_PARAMS['alpha'] = 0.1  # effective learning rate telling us how to weight current return against value already in table\n",
        "MY_HYPER_PARAMS['max_episode_length'] = 50 # max length of episode before forcing a reset in sarsa or q-learning\n",
        "MY_HYPER_PARAMS['num_exploration_episodes'] = 500  # num episodes to force explore before starting to decay epsilon\n",
        "\n",
        "# for the rubric, need to calculate the reward and check whether illegal actions taken based on greedy choice:  epsilon=0\n",
        "# I will therefore force my epsilon to zero after a certain number (num_episodes_with_some_exploration) of episodes.\n",
        "# Will generally want this to be ~ 1100 less than total num episodes so that get at least 1100 episodes for rubric\n",
        "MY_HYPER_PARAMS['num_episodes_with_some_exploration'] = MY_HYPER_PARAMS['n_episodes'] - 1102"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH-ljA46XZ2J"
      },
      "source": [
        "I preferred to work on my local machine in PyCharm for the enhanced debugging capabilities offered by that IDE.  My hyperparameter values resulted largely from trial-and-error in PyCharm, largely with the Q-learning algorithm, by inspection of the plot of windowed_rewards (see below) and the rubric values (e.g., 5th and 95th percentile rewards) for different hyperparameter settings.  For example:\n",
        "- I found that reducing `alpha` below 0.1 reduced convergence speed (e.g., I had to run for more episodes to get same result) without improving performance, so I am setting `alpha` to 0.1.\n",
        "- The `epsilon_decay value` of 0.99 implies that once we start allowing decay (after an initial exploration period of `num_exploration_episodes=500`) , it takes log(`min_epsilon`)/log(0.99) ~ 300 episodes to transition from full exploration to almost full exploitation. \n",
        "- Since there are 25 locations on the MAP, and we need to both pick up and dropoff the passenger, it seems reasonable to set the `max_episode_length` to 50.\n",
        "- The `num_episodes_with_some_exploration` is not really a hyperparameter.  I added it to force epsilon to 0 for the last 1100 episodes so that the last 1100 values in the vectors used for the rubric (e.g., the rewards vector and vector verifying that no illegal actions are taken) reflect episodes based entirely on greedy actions.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRXtsYHFbSv-",
        "outputId": "e92c7e75-d285-4125-99a5-fc9975d1dac2"
      },
      "source": [
        "# Define the MAP, initialize the taxi environment, and check that we can reset and render\n",
        "\n",
        "MAP = [\n",
        "    \"+---------+\",\n",
        "    \"|R: | : :G|\",\n",
        "    \"| : | : : |\",\n",
        "    \"| : : : : |\",\n",
        "    \"| | : | : |\",\n",
        "    \"|Y| : |B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "global_taxi = gym.make('Taxi-v3')\n",
        "global_taxi.reset()\n",
        "global_taxi.render()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "|\u001b[43m \u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "9AtsPln5dlmY",
        "outputId": "4f69d96f-53c8-43fa-bb25-86ac1df4939c"
      },
      "source": [
        "# Run Q-learning and check rubrics:\n",
        "estimated_returns_tbl, rewards, had_illegal_action = q_learning(global_taxi, MY_HYPER_PARAMS)\n",
        "\n",
        "# Calculate a moving average over a sliding window of 100 episodes \n",
        "windowed_rewards = np.convolve(rewards, np.ones(100)/100, 'valid')\n",
        "plt.plot(windowed_rewards[500:])\n",
        "plt.show()\n",
        "\n",
        "last_1000_start_idx = MY_HYPER_PARAMS['n_episodes'] - 100 - 1000\n",
        "avg_reward_last_1000 = windowed_rewards[last_1000_start_idx:(last_1000_start_idx+999)]\n",
        "\n",
        "avg_reward_5p = np.quantile(avg_reward_last_1000,.05)\n",
        "avg_reward_95p = np.quantile(avg_reward_last_1000,.95)\n",
        "\n",
        "print('5th percentile = '+str(avg_reward_5p))\n",
        "print('95th percentile = '+str(avg_reward_95p))\n",
        "print('Avg reward = '+str(np.mean(avg_reward_last_1000)))\n",
        "\n",
        "illegal_actions = np.count_nonzero(had_illegal_action[last_1000_start_idx:])\n",
        "print('Illegal actions in last 1000 = '+str(illegal_actions))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [02:52<00:00, 58.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "5th percentile = 7.38\n",
            "95th percentile = 8.45\n",
            "Avg reward = 7.884304304304305\n",
            "Illegal actions in last 1000 = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4hZq0F6fRSH"
      },
      "source": [
        "As seen above:\n",
        "- the 5th percentile is 7.38>7.2\n",
        "- the 95th percentile is 8.45>8.2\n",
        "- no illegal actions are taken in the last 1000 episodes"
      ]
    }
  ]
}